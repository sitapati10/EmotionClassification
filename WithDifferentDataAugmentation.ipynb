{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpg7Z0t69s4L",
        "outputId": "92fdb953-2d0c-451d-fbd5-b74b8ad5bee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/AudioWAV.zip' has been successfully extracted to 'Audio'.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_folder(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Specify the path to the zip file and the directory where you want to extract the contents\n",
        "zip_file_path = '/content/drive/MyDrive/AudioWAV.zip'\n",
        "extracted_folder_path = 'Audio'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Call the function to unzip the folder\n",
        "unzip_folder(zip_file_path, extracted_folder_path)\n",
        "\n",
        "print(f\"Folder '{zip_file_path}' has been successfully extracted to '{extracted_folder_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sgol7L6H-Rx1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Specify the path for the new folder\n",
        "folder_path = \"/content/AudioWithCategorisedWAV/\"\n",
        "\n",
        "folders = ['Anger','Disgust','Fear','Happy','Neutral','Sad']\n",
        "\n",
        "for i in folders:\n",
        "  folder_path1 = folder_path + i\n",
        "  if not os.path.exists(folder_path1):\n",
        "    os.makedirs(folder_path1)\n",
        "\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "#sourcepath = '/content/AudioInput'\n",
        "sourcepath = '/content/Audio/AudioWAV'\n",
        "destinationpath = '/content/AudioWithCategorisedWAV'\n",
        "files = os.listdir(sourcepath)\n",
        "\n",
        "for file in files:\n",
        "    source_file = os.path.join(sourcepath, file)\n",
        "    if not '.wav' in file:\n",
        "      continue\n",
        "    if 'ANG' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Anger', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'DIS' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Disgust', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'FEA' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Fear', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'HAP' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Happy', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'NEU' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Neutral', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'SAD' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Sad', file)\n",
        "      shutil.move(source_file, destination_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3B_PdEqB-YKg"
      },
      "outputs": [],
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.70):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.8):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "def higher_speed(data, speed_factor = 1.25):\n",
        "    return librosa.effects.time_stretch(data, rate = speed_factor)\n",
        "\n",
        "def lower_speed(data, speed_factor = 0.75):\n",
        "    return librosa.effects.time_stretch(data, rate = speed_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep0IoBxv-a_G",
        "outputId": "e7af546d-867c-42cc-fd33-ecd8421d3152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
            "  return pitch_tuning(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to extract audio features using librosa\n",
        "def extract_features(audio, sample_rate, mfcc=True, chroma=True, mel=True):\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, mel))\n",
        "    return result\n",
        "\n",
        "# Function to load audio data and labels\n",
        "def load_data(data_path):\n",
        "    features, labels = [], []\n",
        "    for folder in os.listdir(data_path):\n",
        "        label = folder\n",
        "        for file_name in os.listdir(os.path.join(data_path, folder)):\n",
        "            file_path = os.path.join(data_path, folder, file_name)\n",
        "            audio, sample_rate = librosa.load(file_path)\n",
        "            feature = extract_features(audio,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "            #noised\n",
        "            noise_data = noise(audio)\n",
        "            feature = extract_features(noise_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "            #stretched\n",
        "            #stretch_data = stretch(audio)\n",
        "            #feature = extract_features(stretch_data,sample_rate)\n",
        "            #features.append(feature)\n",
        "            #labels.append(label)\n",
        "            #pitched\n",
        "            #pitch_data = pitch(data = audio, sampling_rate = sample_rate)\n",
        "            #feature = extract_features(pitch_data,sample_rate)\n",
        "            #features.append(feature)\n",
        "            #labels.append(label)\n",
        "\n",
        "            #speed up\n",
        "            higher_speed_data = higher_speed(audio)\n",
        "            feature = extract_features(higher_speed_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "\n",
        "            #speed down\n",
        "            lower_speed_data = higher_speed(audio)\n",
        "            feature = extract_features(lower_speed_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load data and preprocess\n",
        "data_path = \"/content/AudioWithCategorisedWAV\"\n",
        "features, labels = load_data(data_path)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lfk3lrdh-cFi"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.1, random_state=42)\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(layers.Conv1D(256, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Recurrent layers\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpMNf1At-dxS",
        "outputId": "d4d22735-8486-42b8-ebbb-10beab49de75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "838/838 [==============================] - 123s 139ms/step - loss: 1.4771 - accuracy: 0.3864 - val_loss: 1.4447 - val_accuracy: 0.4044\n",
            "Epoch 2/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 1.3835 - accuracy: 0.4352 - val_loss: 1.7303 - val_accuracy: 0.3342\n",
            "Epoch 3/50\n",
            "838/838 [==============================] - 115s 138ms/step - loss: 1.3291 - accuracy: 0.4643 - val_loss: 1.5542 - val_accuracy: 0.3581\n",
            "Epoch 4/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 1.2833 - accuracy: 0.4877 - val_loss: 1.4014 - val_accuracy: 0.4071\n",
            "Epoch 5/50\n",
            "838/838 [==============================] - 117s 139ms/step - loss: 1.2288 - accuracy: 0.5125 - val_loss: 1.6365 - val_accuracy: 0.3490\n",
            "Epoch 6/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 1.1669 - accuracy: 0.5379 - val_loss: 1.5838 - val_accuracy: 0.4353\n",
            "Epoch 7/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 1.0950 - accuracy: 0.5723 - val_loss: 1.6076 - val_accuracy: 0.4330\n",
            "Epoch 8/50\n",
            "838/838 [==============================] - 117s 140ms/step - loss: 1.0163 - accuracy: 0.6043 - val_loss: 1.1308 - val_accuracy: 0.5536\n",
            "Epoch 9/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.9144 - accuracy: 0.6454 - val_loss: 1.3760 - val_accuracy: 0.4914\n",
            "Epoch 10/50\n",
            "838/838 [==============================] - 120s 143ms/step - loss: 0.8097 - accuracy: 0.6880 - val_loss: 1.3001 - val_accuracy: 0.5217\n",
            "Epoch 11/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.6967 - accuracy: 0.7357 - val_loss: 0.9842 - val_accuracy: 0.6500\n",
            "Epoch 12/50\n",
            "838/838 [==============================] - 117s 139ms/step - loss: 0.5983 - accuracy: 0.7744 - val_loss: 1.2061 - val_accuracy: 0.6127\n",
            "Epoch 13/50\n",
            "838/838 [==============================] - 118s 141ms/step - loss: 0.5092 - accuracy: 0.8108 - val_loss: 1.3963 - val_accuracy: 0.5042\n",
            "Epoch 14/50\n",
            "838/838 [==============================] - 118s 140ms/step - loss: 0.4363 - accuracy: 0.8392 - val_loss: 0.8664 - val_accuracy: 0.7138\n",
            "Epoch 15/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.3786 - accuracy: 0.8627 - val_loss: 1.3447 - val_accuracy: 0.5764\n",
            "Epoch 16/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.3144 - accuracy: 0.8867 - val_loss: 1.2315 - val_accuracy: 0.6312\n",
            "Epoch 17/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.2806 - accuracy: 0.8983 - val_loss: 0.6582 - val_accuracy: 0.7843\n",
            "Epoch 18/50\n",
            "838/838 [==============================] - 122s 146ms/step - loss: 0.2518 - accuracy: 0.9088 - val_loss: 0.6921 - val_accuracy: 0.7807\n",
            "Epoch 19/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.2221 - accuracy: 0.9206 - val_loss: 1.1201 - val_accuracy: 0.6826\n",
            "Epoch 20/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.2002 - accuracy: 0.9275 - val_loss: 2.2237 - val_accuracy: 0.5946\n",
            "Epoch 21/50\n",
            "838/838 [==============================] - 117s 140ms/step - loss: 0.2004 - accuracy: 0.9306 - val_loss: 1.0732 - val_accuracy: 0.7232\n",
            "Epoch 22/50\n",
            "838/838 [==============================] - 119s 142ms/step - loss: 0.1556 - accuracy: 0.9449 - val_loss: 1.2612 - val_accuracy: 0.7121\n",
            "Epoch 23/50\n",
            "838/838 [==============================] - 116s 139ms/step - loss: 0.1600 - accuracy: 0.9454 - val_loss: 2.2197 - val_accuracy: 0.5774\n",
            "Epoch 24/50\n",
            "838/838 [==============================] - 119s 141ms/step - loss: 0.1555 - accuracy: 0.9475 - val_loss: 0.6720 - val_accuracy: 0.8193\n",
            "Epoch 25/50\n",
            "838/838 [==============================] - 115s 138ms/step - loss: 0.1445 - accuracy: 0.9502 - val_loss: 1.6064 - val_accuracy: 0.6426\n",
            "Epoch 26/50\n",
            "838/838 [==============================] - 116s 139ms/step - loss: 0.1387 - accuracy: 0.9536 - val_loss: 0.9773 - val_accuracy: 0.7531\n",
            "Epoch 27/50\n",
            "838/838 [==============================] - 116s 139ms/step - loss: 0.1304 - accuracy: 0.9552 - val_loss: 0.7016 - val_accuracy: 0.8280\n",
            "Epoch 28/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.1138 - accuracy: 0.9610 - val_loss: 0.6423 - val_accuracy: 0.8381\n",
            "Epoch 29/50\n",
            "838/838 [==============================] - 119s 142ms/step - loss: 0.1188 - accuracy: 0.9597 - val_loss: 1.6176 - val_accuracy: 0.6654\n",
            "Epoch 30/50\n",
            "838/838 [==============================] - 117s 139ms/step - loss: 0.1136 - accuracy: 0.9628 - val_loss: 2.0612 - val_accuracy: 0.6241\n",
            "Epoch 31/50\n",
            "838/838 [==============================] - 117s 139ms/step - loss: 0.1070 - accuracy: 0.9633 - val_loss: 1.2078 - val_accuracy: 0.7400\n",
            "Epoch 32/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.1151 - accuracy: 0.9625 - val_loss: 3.8482 - val_accuracy: 0.4185\n",
            "Epoch 33/50\n",
            "838/838 [==============================] - 115s 138ms/step - loss: 0.0954 - accuracy: 0.9679 - val_loss: 0.6961 - val_accuracy: 0.8257\n",
            "Epoch 34/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.1047 - accuracy: 0.9653 - val_loss: 0.5808 - val_accuracy: 0.8535\n",
            "Epoch 35/50\n",
            "838/838 [==============================] - 119s 142ms/step - loss: 0.0948 - accuracy: 0.9683 - val_loss: 2.3579 - val_accuracy: 0.5805\n",
            "Epoch 36/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.0931 - accuracy: 0.9696 - val_loss: 0.9844 - val_accuracy: 0.7927\n",
            "Epoch 37/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0965 - accuracy: 0.9680 - val_loss: 1.2907 - val_accuracy: 0.7256\n",
            "Epoch 38/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.0871 - accuracy: 0.9712 - val_loss: 0.6876 - val_accuracy: 0.8418\n",
            "Epoch 39/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.0888 - accuracy: 0.9701 - val_loss: 2.4372 - val_accuracy: 0.6120\n",
            "Epoch 40/50\n",
            "838/838 [==============================] - 113s 135ms/step - loss: 0.0946 - accuracy: 0.9681 - val_loss: 2.9159 - val_accuracy: 0.5673\n",
            "Epoch 41/50\n",
            "838/838 [==============================] - 116s 138ms/step - loss: 0.0753 - accuracy: 0.9744 - val_loss: 1.3704 - val_accuracy: 0.7168\n",
            "Epoch 42/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0866 - accuracy: 0.9727 - val_loss: 3.0268 - val_accuracy: 0.5435\n",
            "Epoch 43/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0876 - accuracy: 0.9710 - val_loss: 0.7608 - val_accuracy: 0.8200\n",
            "Epoch 44/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0739 - accuracy: 0.9756 - val_loss: 0.5243 - val_accuracy: 0.8710\n",
            "Epoch 45/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0764 - accuracy: 0.9748 - val_loss: 2.2919 - val_accuracy: 0.6466\n",
            "Epoch 46/50\n",
            "838/838 [==============================] - 116s 139ms/step - loss: 0.0711 - accuracy: 0.9758 - val_loss: 0.6237 - val_accuracy: 0.8582\n",
            "Epoch 47/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0869 - accuracy: 0.9715 - val_loss: 0.6792 - val_accuracy: 0.8401\n",
            "Epoch 48/50\n",
            "838/838 [==============================] - 115s 137ms/step - loss: 0.0683 - accuracy: 0.9775 - val_loss: 0.9881 - val_accuracy: 0.7860\n",
            "Epoch 49/50\n",
            "838/838 [==============================] - 114s 136ms/step - loss: 0.0667 - accuracy: 0.9776 - val_loss: 2.1961 - val_accuracy: 0.6460\n",
            "Epoch 50/50\n",
            "838/838 [==============================] - 115s 138ms/step - loss: 0.0724 - accuracy: 0.9758 - val_loss: 0.7479 - val_accuracy: 0.8290\n",
            "94/94 [==============================] - 4s 46ms/step - loss: 0.7479 - accuracy: 0.8290\n",
            "Test accuracy: 0.8290225267410278\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_labels = np.unique(labels)\n",
        "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "Y = np.array([class_indices[label] for label in labels])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight ='balanced',classes = np.unique(Y),y= Y)\n",
        "\n",
        "# Convert class weights to a dictionary for class_weight parameter in model.fit\n",
        "class_weights_dict = {class_index: weight for class_index, weight in zip(np.unique(Y), class_weights)}\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weights_dict)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JwrPodRPZSu",
        "outputId": "46722a98-f37a-4332-b500-c97350cfa29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 4s 45ms/step - loss: 0.9690 - accuracy: 0.7823\n",
            "Test accuracy: 0.7823312282562256\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HK9Q8PiaF7eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLp51yqvNRT5",
        "outputId": "a86f094b-5599-41b5-82fc-c3610f67737b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"emotion_classification_Audio_model_With_Augmentation_With_More_Features.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZh89tr0-f0s"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Create an augmentation pipeline\n",
        "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
        "sentdict = {'IEO':\"It's eleven o'clock\",'TIE':\"That is exactly what happened\",'IOM':\"I'm on my way to the meeting\",'IWW':\"I wonder what this is about\",'TAI':\"The airplane is almost full\",'MTI':\"Maybe tomorrow it will be cold\",\n",
        "            'IWL':\"I would like a new alarm clock\",'ITH':\"I think I have a doctor's appointment\",'DFA':\"Don't forget a jacket\",'ITS':\"I think I've seen this before\",'TSI':\"The surface is slick\",'WSI':\"We'll stop in a couple of minutes\"}\n",
        "testsentences = []\n",
        "finalemotion=[]\n",
        "def create(sentence):\n",
        "  text = sentence + ' üò†'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(0)\n",
        "  finalemotion.append(0)\n",
        "  finalemotion.append(0)\n",
        "  text = sentence + ' üòñ'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(1)\n",
        "  finalemotion.append(1)\n",
        "  finalemotion.append(1)\n",
        "  text = sentence + ' üò±'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(2)\n",
        "  finalemotion.append(2)\n",
        "  finalemotion.append(2)\n",
        "  text = sentence + ' üòä'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(3)\n",
        "  finalemotion.append(3)\n",
        "  finalemotion.append(3)\n",
        "  text = sentence + ' üòê'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(4)\n",
        "  finalemotion.append(4)\n",
        "  finalemotion.append(4)\n",
        "  text = sentence + ' üò¢'\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(5)\n",
        "  finalemotion.append(5)\n",
        "  finalemotion.append(5)\n",
        "for k in sentdict.keys():\n",
        "  create(sentdict[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx8gvxsIRDmR",
        "outputId": "e94ac7bd-a109-4525-ce68-1ec1d123b09c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216\n",
            "216\n"
          ]
        }
      ],
      "source": [
        "print(len(testsentences))\n",
        "print(len(finalemotion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bA50DewLHJw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "augmentedemojitext=[]\n",
        "emotions=[]\n",
        "\n",
        "def emoji_augmentation(text):\n",
        "    # Define a dictionary of emoji replacements\n",
        "    emoji_replacements = {\n",
        "        \"üò†\": [\"üò†\", \"üò°\", \"üò§\", \"üòæ\"],\n",
        "        \"üòñ\": [\"üòñ\", \"üò£\", \"üòû\", \"üò∑\"],\n",
        "        \"üò±\": [\"üò±\", \"üò®\", \"üò∞\", \"üò≤\"],\n",
        "        \"üòä\": [\"üòä\", \"üòÑ\", \"üòÅ\", \"üòÜ\"],\n",
        "        \"üòê\": [\"üòê\", \"üòë\", \"üò∂\", \"üòè\"],\n",
        "        \"üò¢\": [\"üò¢\", \"üò≠\", \"üòì\", \"üò•\"],\n",
        "        # Add more emojis and their possible replacements\n",
        "    }\n",
        "\n",
        "    emoji_dict = {\"üò†\":0,\"üòñ\": 1,\"üò±\": 2,\"üòä\": 3,\"üòê\": 4,\"üò¢\": 5}\n",
        "\n",
        "    # Use regular expression to find emojis in the text\n",
        "    emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+')\n",
        "    matches = emoji_pattern.findall(text)\n",
        "\n",
        "    # Perform augmentation by randomly replacing emojis\n",
        "    for match in matches:\n",
        "      if match in emoji_replacements:\n",
        "        for replacement in emoji_replacements[match]:\n",
        "          augmented_text = text\n",
        "          augmented_text = augmented_text.replace(match, replacement)\n",
        "          augmentedemojitext.append(augmented_text)\n",
        "          emotions.append(emoji_dict[match])\n",
        "\n",
        "for text in testsentences:\n",
        "  emoji_augmentation(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkRk7OgwNOz2",
        "outputId": "799a4851-59a1-45c0-8184-d5140cc2f199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864\n",
            "216\n"
          ]
        }
      ],
      "source": [
        "print(len(augmentedemojitext))\n",
        "print(len(finalemotion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcrbtNeaSAvh"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from transformers import TFRobertaModel, RobertaTokenizerFast\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tabulate import tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSBBl4OVRKKa"
      },
      "outputs": [],
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(augmentedemojitext,emotions, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uhjXURASFQM",
        "outputId": "55c328ed-044e-4ac6-8c96-b83a0d3619c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion were not used when initializing TFRobertaModel: ['classifier']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFRobertaModel, RobertaTokenizerFast\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "\n",
        "MAX_LEN=128\n",
        "\n",
        "def tokenize_roberta(data, max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    inputs = Input(shape=(max_len,), dtype='int32')\n",
        "    masks = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "    bert_output = bert_model([inputs, masks])[1]\n",
        "\n",
        "    dense_1 = Dense(128, activation='relu')(bert_output)\n",
        "    dropout_1 = Dropout(0.5)(dense_1)\n",
        "\n",
        "    dense_2 = Dense(64, activation='relu')(dropout_1)\n",
        "    dropout_2 = Dropout(0.5)(dense_2)\n",
        "\n",
        "    output = Dense(6, activation='softmax')(dropout_2)\n",
        "\n",
        "    model = Model(inputs=[inputs, masks], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "                  loss=CategoricalCrossentropy(),\n",
        "                  metrics=CategoricalAccuracy())\n",
        "    return model\n",
        "\n",
        "roberta_model = TFRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "model = create_model(roberta_model, MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh2NHcZwSMgq",
        "outputId": "eda40fb6-0e23-48e1-8f5c-9f6ab994e185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "25/25 [==============================] - 64s 751ms/step - loss: 1.8950 - categorical_accuracy: 0.1905\n",
            "Epoch 2/4\n",
            "25/25 [==============================] - 18s 738ms/step - loss: 1.6480 - categorical_accuracy: 0.2921\n",
            "Epoch 3/4\n",
            "25/25 [==============================] - 19s 742ms/step - loss: 1.4241 - categorical_accuracy: 0.4350\n",
            "Epoch 4/4\n",
            "25/25 [==============================] - 19s 751ms/step - loss: 1.2489 - categorical_accuracy: 0.5084\n"
          ]
        }
      ],
      "source": [
        "train_inputs, train_masks = tokenize_roberta(X_train1, MAX_LEN)#tokenize_roberta(X_train, MAX_LEN)\n",
        "history = model.fit([train_inputs, train_masks],  OneHotEncoder().fit_transform(np.array(y_train1).reshape(-1, 1)).toarray(),  epochs=4,  batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3H4oFbASRvr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "textemo=[]\n",
        "for s in X_test1:\n",
        "  textemo.append(np.argmax(model.predict([tokenize_roberta([s])])[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2R-wSkXSau-",
        "outputId": "cfcafd0e-687b-43ab-d4cf-e1ca161d4ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9080459770114943\n"
          ]
        }
      ],
      "source": [
        "te=0\n",
        "for i in range(0,len(X_test1)):\n",
        "  if y_test1[i] == textemo[i]:\n",
        "    te= te+1\n",
        "print(te/len(X_test1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUJyuQdypBTU"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "Audio_model = load_model(\"/content/emotion_classification_Audio_model_With_Augmentation.h5\")\n",
        "Video_model = load_model(\"/content/drive/MyDrive/emotion_classification_Video_model.h5\")\n",
        "text_model = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nWppSxNkubmu",
        "outputId": "bf69b333-c66b-4903-e430-755ce0de2b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/drive/MyDrive/VideoFlash.zip' has been successfully extracted to 'Video'.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_folder(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Specify the path to the zip file and the directory where you want to extract the contents\n",
        "zip_file_path = '/content/drive/MyDrive/VideoFlash.zip'\n",
        "extracted_folder_path = 'Video'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Call the function to unzip the folder\n",
        "unzip_folder(zip_file_path, extracted_folder_path)\n",
        "\n",
        "print(f\"Folder '{zip_file_path}' has been successfully extracted to '{extracted_folder_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ijr2x4ar7tu",
        "outputId": "7650dfed-88c5-4efe-8370-6620c63a944f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7442\n",
            "7442\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "VideosData = []\n",
        "data_path = '/content/Video/VideoFlash'\n",
        "for folder in os.listdir(data_path):\n",
        "    file_path = os.path.join(data_path, folder)\n",
        "    VideosData.append(folder)\n",
        "\n",
        "AudiosData = []\n",
        "data_path = '/content/Audio/AudioWAV'\n",
        "for folder in os.listdir(data_path):\n",
        "    file_path = os.path.join(data_path, folder)\n",
        "    AudiosData.append(folder)\n",
        "\n",
        "print(len(VideosData))\n",
        "print(len(AudiosData))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMOy78o2sVkX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "vd, VideosDataTest = train_test_split(VideosData, test_size=0.4, random_state=42)\n",
        "ad, AudiosDataTest = train_test_split(AudiosData, test_size=0.4, random_state=42)\n",
        "testdata=[]\n",
        "for data in VideosDataTest:\n",
        "  if data[:-3]+'wav' in AudiosDataTest:\n",
        "    testdata.append(data[:-3])\n",
        "\n",
        "for data in AudiosDataTest:\n",
        "  if data[:-3]+'flv' in VideosDataTest:\n",
        "    testdata.append(data[:-3])\n",
        "\n",
        "testdata = set(testdata)\n",
        "testdata = list(testdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40u2Obn7O7Hf"
      },
      "outputs": [],
      "source": [
        "finalemotion = []\n",
        "for file in testdata:\n",
        "  if 'ANG' in file:\n",
        "    finalemotion.append(0)\n",
        "  elif 'DIS' in file:\n",
        "    finalemotion.append(1)\n",
        "  elif 'FEA' in file:\n",
        "    finalemotion.append(2)\n",
        "  elif 'HAP' in file:\n",
        "    finalemotion.append(3)\n",
        "  elif 'NEU' in file:\n",
        "    finalemotion.append(4)\n",
        "  elif 'SAD' in file:\n",
        "    finalemotion.append(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfL-6GUvwm1H",
        "outputId": "ca4685cb-0ce6-4808-896f-dd4b8ecfa8c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1216"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(testdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8N-IZ7ZsvHL"
      },
      "outputs": [],
      "source": [
        "emoji_replacements = {\n",
        "        \"üò†\": [\"üò†\", \"üò°\", \"üò§\", \"üòæ\"],\n",
        "        \"üòñ\": [\"üòñ\", \"üò£\", \"üòû\", \"üò∑\"],\n",
        "        \"üò±\": [\"üò±\", \"üò®\", \"üò∞\", \"üò≤\"],\n",
        "        \"üòä\": [\"üòä\", \"üòÑ\", \"üòÅ\", \"üòÜ\"],\n",
        "        \"üòê\": [\"üòê\", \"üòë\", \"üò∂\", \"üòè\"],\n",
        "        \"üò¢\": [\"üò¢\", \"üò≠\", \"üòì\", \"üò•\"],\n",
        "        # Add more emojis and their possible replacements\n",
        "    }\n",
        "def CreateSentences(file,sentence,testsentences):\n",
        "  if 'ANG' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üò†']))\n",
        "  elif 'DIS' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üòñ']))\n",
        "  elif 'FEA' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üò±']))\n",
        "  elif 'HAP' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üòä']))\n",
        "  elif 'NEU' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üòê']))\n",
        "  elif 'SAD' in file:\n",
        "    testsentences.append(sentence + random.choice(emoji_replacements['üò¢']))\n",
        "\n",
        "testsentences=[]\n",
        "sentdict = {'IEO':\"It's eleven o'clock\",'TIE':\"That is exactly what happened\",'IOM':\"I'm on my way to the meeting\",'IWW':\"I wonder what this is about\",'TAI':\"The airplane is almost full\",'MTI':\"Maybe tomorrow it will be cold\",\n",
        "            'IWL':\"I would like a new alarm clock\",'ITH':\"I think I have a doctor's appointment\",'DFA':\"Don't forget a jacket\",'ITS':\"I think I've seen this before\",'TSI':\"The surface is slick\",'WSI':\"We'll stop in a couple of minutes\"}\n",
        "for file in testdata :\n",
        "  CreateSentences(file,sentdict[file[5:8]],testsentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900h_Qy1r0UI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import librosa\n",
        "\n",
        "# Function to extract features from video\n",
        "def extract_video_features(video_path):\n",
        "    # Your video feature extraction code here\n",
        "    # Example: Using OpenCV to extract color histogram features\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        processed_frame = cv2.resize(frame, (224, 224))\n",
        "        processed_frame = processed_frame / 255.0  # Normalize pixel values\n",
        "\n",
        "    cap.release()\n",
        "    return processed_frame\n",
        "\n",
        "# Function to extract features from audio\n",
        "def extract_audio_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    # Your audio feature extraction code here\n",
        "    # Example: Using librosa to extract MFCC features\n",
        "    audio, sample_rate = librosa.load(file_path)\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    #if chroma:\n",
        "    #    chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate), axis=1)\n",
        "    #    result = np.hstack((result, chroma))\n",
        "    #if mel:\n",
        "    #    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate), axis=1)\n",
        "    #    result = np.hstack((result, mel))\n",
        "    return result\n",
        "\n",
        "# Load your pre-trained text model (replace with your actual model loading code)\n",
        "def predict_text_emotion(text):\n",
        "    # Your text prediction code here\n",
        "    # Example: Using a simple RandomForestClassifier\n",
        "    # You should replace this with your actual text classification model\n",
        "    return text_model.predict([tokenize_roberta([text])])[0]\n",
        "\n",
        "# Load your pre-trained video model (replace with your actual model loading code)\n",
        "def predict_video_emotion(video_path):\n",
        "    # Your video prediction code here\n",
        "    # Example: Extract video features and use a simple RandomForestClassifier\n",
        "    video_features = extract_video_features(video_path)\n",
        "    return Video_model.predict(np.array([video_features]))\n",
        "\n",
        "# Load your pre-trained audio model (replace with your actual model loading code)\n",
        "def predict_audio_emotion(audio_path):\n",
        "    # Your audio prediction code here\n",
        "    # Example: Extract audio features and use a simple RandomForestClassifier\n",
        "    audio_features = extract_audio_features(audio_path)\n",
        "    return Audio_model.predict(np.array([audio_features]))\n",
        "\n",
        "textemotion_pred=[]\n",
        "videoemotion_pred=[]\n",
        "audioemotion_pred=[]\n",
        "# Example usage\n",
        "for i in range(0,len(testdata)):\n",
        "  text = testsentences[i]\n",
        "  video_path = '/content/Video/VideoFlash/' + testdata[i] + 'flv' #\"/content/Video/1064_IEO_SAD_HI.flv\"\n",
        "  audio_path = '/content/Audio/AudioWAV/'+ testdata[i] + 'wav' #\"/content/Audio/1064_IEO_SAD_HI.wav\"\n",
        "\n",
        "    # Let's assume your text, video, and audio models have predicted the following emotions\n",
        "  textemotion_pred.append(predict_text_emotion(text))#[0.2, 0.3, 0.1, 0.1, 0.2, 0.1]  # Example prediction from the text model\n",
        "  videoemotion_pred.append(predict_video_emotion(video_path))#[0.1, 0.4, 0.1, 0.1, 0.1, 0.2]  # Example prediction from the video model\n",
        "  audioemotion_pred.append(predict_audio_emotion(audio_path))#[0.3, 0.2, 0.2, 0.1, 0.1, 0.1]  # Example prediction from the audio model\n",
        "\n",
        "textemotions=[]\n",
        "videoemotions=[]\n",
        "audioemotions=[]\n",
        "finalemotions=[]\n",
        "def predict(tw,vw,aw):\n",
        "  textemotions.clear()\n",
        "  videoemotions.clear()\n",
        "  audioemotions.clear()\n",
        "  finalemotions.clear()\n",
        "  for i in range(0,len(testdata)):\n",
        "    # Set custom weights for each modality\n",
        "    text_weight = tw\n",
        "    video_weight = vw\n",
        "    audio_weight = aw\n",
        "\n",
        "    # Apply custom weights to each modality's prediction\n",
        "    weighted_text_emotion = text_weight * np.array(textemotion_pred[i])\n",
        "    weighted_video_emotion = video_weight * np.array(videoemotion_pred[i])\n",
        "    weighted_audio_emotion = audio_weight * np.array(audioemotion_pred[i])\n",
        "\n",
        "    # Combine the weighted predictions (you can choose a different method, e.g., averaging)\n",
        "    final_emotion = np.argmax(weighted_text_emotion + weighted_video_emotion + weighted_audio_emotion)\n",
        "\n",
        "    textemotions.append(np.argmax(weighted_text_emotion))\n",
        "    videoemotions.append(np.argmax(weighted_video_emotion))\n",
        "    audioemotions.append(np.argmax(weighted_audio_emotion))\n",
        "    finalemotions.append(final_emotion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKo370ynw9xR"
      },
      "outputs": [],
      "source": [
        "#equal weightage to audio and video\n",
        "def getAccuracy():\n",
        "  te=0\n",
        "  ae=0\n",
        "  ve=0\n",
        "  fe=0\n",
        "  for i in range(0,1216):\n",
        "    if finalemotion[i] == textemotions[i]:\n",
        "      te= te+1\n",
        "    if finalemotion[i] == audioemotions[i]:\n",
        "      ae= ae+1\n",
        "    if finalemotion[i] == videoemotions[i]:\n",
        "      ve= ve+1\n",
        "    if finalemotion[i] == finalemotions[i]:\n",
        "      fe= fe+1\n",
        "  print(' final emotion : ' + str(fe/1216))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK0Fu_nUY63x",
        "outputId": "6bfe3bcc-e7e4-49aa-e525-cc69d5fc2e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " final emotion : 0.9712171052631579\n",
            " final emotion : 0.9358552631578947\n",
            " final emotion : 0.625\n",
            " final emotion : 0.9498355263157895\n"
          ]
        }
      ],
      "source": [
        "predict(1,1,1)\n",
        "getAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ06F6yOxaFV",
        "outputId": "a7844ae9-ef41-48af-cd56-319c2481b498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " final emotion : 0.9761513157894737\n",
            " final emotion : 0.8338815789473685\n",
            " final emotion : 0.625\n",
            " final emotion : 0.9498355263157895\n"
          ]
        }
      ],
      "source": [
        "predict(1,1,1)\n",
        "getAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbYqtxsixcDb",
        "outputId": "52bb6c35-5f72-4165-9af1-94e15fb7bc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " final emotion : 0.9712171052631579\n",
            " final emotion : 0.953125\n",
            " final emotion : 0.9588815789473685\n",
            " final emotion : 0.959703947368421\n",
            " final emotion : 0.8634868421052632\n",
            " final emotion : 0.7976973684210527\n",
            " final emotion : 0.7532894736842105\n",
            " final emotion : 0.962171052631579\n",
            " final emotion : 0.959703947368421\n",
            " final emotion : 0.9580592105263158\n",
            " final emotion : 0.7006578947368421\n",
            " final emotion : 0.671875\n",
            " final emotion : 0.6661184210526315\n",
            " final emotion : 0.975328947368421\n",
            " final emotion : 0.9761513157894737\n",
            " final emotion : 0.9736842105263158\n",
            " final emotion : 0.9506578947368421\n",
            " final emotion : 0.9004934210526315\n",
            " final emotion : 0.8379934210526315\n",
            " final emotion : 0.9761513157894737\n",
            " final emotion : 0.977796052631579\n",
            " final emotion : 0.9786184210526315\n",
            " final emotion : 0.9095394736842105\n",
            " final emotion : 0.9243421052631579\n",
            " final emotion : 0.9391447368421053\n",
            " final emotion : 0.9761513157894737\n",
            " final emotion : 0.9802631578947368\n",
            " final emotion : 0.9810855263157895\n",
            " final emotion : 0.9383223684210527\n",
            " final emotion : 0.9736842105263158\n",
            " final emotion : 0.9728618421052632\n",
            " final emotion : 0.884046052631579\n",
            " final emotion : 0.96875\n",
            " final emotion : 0.9391447368421053\n"
          ]
        }
      ],
      "source": [
        "predict(1,1,1)\n",
        "getAccuracy()\n",
        "predict(0,1,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(0,1,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(0,1,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(0,1.2,1) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(0,1.5,1) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(0,1.8,1) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1,0,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,0,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,0,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.2,0) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.5,0) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.8,0) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.2,1) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.5,1) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.8,1) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1.2,1,1) # tw =1.2\n",
        "getAccuracy()\n",
        "predict(1.5,1,1) # tw =1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1,1) # tw =1.8\n",
        "getAccuracy()\n",
        "predict(1.2,1.5,1) # tw =1.2,vw =1.5\n",
        "getAccuracy()\n",
        "predict(1.5,1.5,1) # tw =1.5,vw = 1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1.5,1) # tw =1.8,vw =1.5\n",
        "getAccuracy()\n",
        "predict(1.2,1,1.5) # tw =1.2\n",
        "getAccuracy()\n",
        "predict(1.5,1,1.5) # tw =1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1,1.5) # tw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.5,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.2,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.2,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.8,1.2) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.5,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.8,1.5) # aw =1.8\n",
        "getAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrGTmI0Vz7Bm",
        "outputId": "9caa5ee1-2430-407c-ab8e-2e4bf1b22667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.99      0.97      0.98       197\n",
            "     Disgust       0.90      0.97      0.93       184\n",
            "        Fear       0.97      0.91      0.94       204\n",
            "       Happy       0.95      0.99      0.97       219\n",
            "     Neutral       0.94      0.96      0.95       206\n",
            "         Sad       0.94      0.90      0.92       206\n",
            "\n",
            "    accuracy                           0.95      1216\n",
            "   macro avg       0.95      0.95      0.95      1216\n",
            "weighted avg       0.95      0.95      0.95      1216\n",
            "\n",
            "Confusion Matrix:\n",
            "[[191   0   2   4   0   0]\n",
            " [  0 179   0   2   2   1]\n",
            " [  0   5 185   1   5   8]\n",
            " [  0   3   0 216   0   0]\n",
            " [  0   3   1   2 198   2]\n",
            " [  1   9   3   2   5 186]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(audioemotions)\n",
        "true_labels = label_encoder.inverse_transform(finalemotion)\n",
        "classification_metrics = classification_report(predicted_labels, true_labels)\n",
        "confusion_mtx = confusion_matrix(predicted_labels, true_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTBaU0ob1n1t",
        "outputId": "6a66eeac-4d85-424c-82f9-fe6c6d364263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.55      0.55      0.55       192\n",
            "     Disgust       0.73      0.64      0.68       229\n",
            "        Fear       0.58      0.57      0.57       196\n",
            "       Happy       0.79      0.89      0.84       203\n",
            "     Neutral       0.63      0.60      0.61       223\n",
            "         Sad       0.43      0.49      0.46       173\n",
            "\n",
            "    accuracy                           0.62      1216\n",
            "   macro avg       0.62      0.62      0.62      1216\n",
            "weighted avg       0.63      0.62      0.63      1216\n",
            "\n",
            "Confusion Matrix:\n",
            "[[105  18  30   9  15  15]\n",
            " [ 13 146  12  18  10  30]\n",
            " [ 31  13 111   8  14  19]\n",
            " [  3   4   2 180   6   8]\n",
            " [ 19   8  14   9 133  40]\n",
            " [ 21  10  22   3  32  85]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(videoemotions)\n",
        "true_labels = label_encoder.inverse_transform(finalemotion)\n",
        "classification_metrics = classification_report(predicted_labels, true_labels)\n",
        "confusion_mtx = confusion_matrix(predicted_labels, true_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GLNWdKw1tVD",
        "outputId": "46d55d92-e817-42c5-c922-f5a36d7ba300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       1.00      1.00      1.00       192\n",
            "     Disgust       0.84      0.78      0.81       213\n",
            "        Fear       0.77      1.00      0.87       148\n",
            "       Happy       1.00      1.00      1.00       227\n",
            "     Neutral       0.99      1.00      0.99       207\n",
            "         Sad       1.00      0.86      0.92       229\n",
            "\n",
            "    accuracy                           0.94      1216\n",
            "   macro avg       0.93      0.94      0.93      1216\n",
            "weighted avg       0.94      0.94      0.94      1216\n",
            "\n",
            "Confusion Matrix:\n",
            "[[192   0   0   0   0   0]\n",
            " [  0 167  43   0   3   0]\n",
            " [  0   0 148   0   0   0]\n",
            " [  0   0   0 227   0   0]\n",
            " [  0   0   0   0 207   0]\n",
            " [  0  32   0   0   0 197]]\n"
          ]
        }
      ],
      "source": [
        "predicted_labels = label_encoder.inverse_transform(textemotions)\n",
        "true_labels = label_encoder.inverse_transform(finalemotion)\n",
        "classification_metrics = classification_report(predicted_labels, true_labels)\n",
        "confusion_mtx = confusion_matrix(predicted_labels, true_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRgjk5Xu1x4d",
        "outputId": "655c3143-aa1e-4d40-fc4f-10bef9bfcaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       1.00      0.97      0.99       197\n",
            "     Disgust       0.94      0.97      0.96       194\n",
            "        Fear       0.95      0.94      0.95       192\n",
            "       Happy       1.00      1.00      1.00       228\n",
            "     Neutral       0.96      0.98      0.97       206\n",
            "         Sad       0.97      0.96      0.96       199\n",
            "\n",
            "    accuracy                           0.97      1216\n",
            "   macro avg       0.97      0.97      0.97      1216\n",
            "weighted avg       0.97      0.97      0.97      1216\n",
            "\n",
            "Confusion Matrix:\n",
            "[[192   0   2   0   2   1]\n",
            " [  0 188   2   0   2   2]\n",
            " [  0   6 181   0   3   2]\n",
            " [  0   1   0 227   0   0]\n",
            " [  0   1   2   0 202   1]\n",
            " [  0   3   4   0   1 191]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(finalemotions)\n",
        "true_labels = label_encoder.inverse_transform(finalemotion)\n",
        "classification_metrics = classification_report(predicted_labels, true_labels)\n",
        "confusion_mtx = confusion_matrix(predicted_labels, true_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by8Idbxf13MJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(['Angry','Disgust','Fear','Happy','Neutral','Sad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prKvXcHOPUoF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
