{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuTVsNjOZIyK"
      },
      "source": [
        "3d CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BV5FMEfZH5N"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
        "\n",
        "# Define the 3D CNN model for video data\n",
        "video_model = Sequential()\n",
        "video_model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', input_shape=(frames, height, width, channels)))\n",
        "video_model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "video_model.add(Flatten())\n",
        "video_model.add(Dense(128, activation='relu'))\n",
        "video_model.add(Dense(num_classes, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2BpCr-GZOnt"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmXboAWkZQBK"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define BERT input and output layers\n",
        "text_input = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"text_input\")\n",
        "text_output = bert_model(text_input)[0]\n",
        "text_output = Dense(128, activation='relu')(text_output)\n",
        "text_output = Dense(num_classes, activation='softmax')(text_output)\n",
        "\n",
        "text_model = Model(inputs=text_input, outputs=text_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM9TgPDEZRIi"
      },
      "source": [
        "MFCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DbNRRI8ZTkq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, Flatten\n",
        "\n",
        "# Define the MFCC model for audio data\n",
        "audio_model = Sequential()\n",
        "audio_model.add(InputLayer(input_shape=(num_mfcc_features, frames)))\n",
        "audio_model.add(Flatten())\n",
        "audio_model.add(Dense(128, activation='relu'))\n",
        "audio_model.add(Dense(num_classes, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4kxQkpVcC_B"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import concatenate, Input, Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Input layers for each modality\n",
        "video_input = Input(shape=(video_data.shape[1],), name=\"video_input\")\n",
        "text_input = Input(shape=(text_data.shape[1],), name=\"text_input\")\n",
        "audio_input = Input(shape=(audio_data.shape[1],), name=\"audio_input\")\n",
        "\n",
        "# User-defined weights for each modality\n",
        "video_weight = 0.25\n",
        "text_weight = 0.50\n",
        "audio_weight = 0.25\n",
        "\n",
        "# Get the outputs from each modality model\n",
        "video_output = video_model(video_input)\n",
        "text_output = text_model(text_input)\n",
        "audio_output = audio_model(audio_input)\n",
        "\n",
        "# Apply user-defined weights to scale the modality outputs\n",
        "scaled_video = Multiply()([video_output, video_weight])\n",
        "scaled_text = Multiply()([text_output, text_weight])\n",
        "scaled_audio = Multiply()([audio_output, audio_weight])\n",
        "\n",
        "# Combine the scaled outputs\n",
        "combined_features = concatenate([scaled_video, scaled_text, scaled_audio], axis=-1)\n",
        "\n",
        "# Add more layers for multi-modal processing if needed\n",
        "# For instance, you can add dense layers or further attention mechanisms\n",
        "\n",
        "# Final classification layer\n",
        "fusion_output = Dense(num_classes, activation='softmax')(combined_features)\n",
        "\n",
        "# Create the fusion model\n",
        "fusion_model = Model(inputs=[video_input, text_input, audio_input], outputs=fusion_output)\n",
        "\n",
        "# Compile and train the fusion model\n",
        "fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "fusion_model.fit([video_data, text_data, audio_data], labels, epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Specify the path for the new folder\n",
        "folder_path = \"/content/AudioWithCategorisedWAV/\"\n",
        "\n",
        "folders = ['Anger','Disgust','Fear','Happy','Neutral','Sad']\n",
        "\n",
        "for i in folders:\n",
        "  folder_path1 = folder_path + i\n",
        "  if not os.path.exists(folder_path1):\n",
        "    os.makedirs(folder_path1)"
      ],
      "metadata": {
        "id": "aGJcjrNI7RVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wDEWdG_Hvx9"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow numpy librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9ont3NLHw7y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import shutil\n",
        "#sourcepath = '/content/AudioInput'\n",
        "sourcepath = '/content/Audio/AudioWAV'\n",
        "destinationpath = '/content/AudioWithCategorisedWAV'\n",
        "files = os.listdir(sourcepath)\n",
        "i=0\n",
        "for file in files:\n",
        "    i+=1\n",
        "    if i == 3200:\n",
        "      break\n",
        "    source_file = os.path.join(sourcepath, file)\n",
        "    if not '.wav' in file:\n",
        "      continue\n",
        "    if 'ANG' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Anger', file)\n",
        "      shutil.copy(source_file, destination_file)\n",
        "    elif 'DIS' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Disgust', file)\n",
        "      shutil.copy(source_file, destination_file)\n",
        "    elif 'FEA' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Fear', file)\n",
        "      shutil.copy(source_file, destination_file)\n",
        "    elif 'HAP' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Happy', file)\n",
        "      shutil.copy(source_file, destination_file)\n",
        "    elif 'NEU' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Neutral', file)\n",
        "      shutil.copy(source_file, destination_file)\n",
        "    elif 'SAD' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Sad', file)\n",
        "      shutil.copy(source_file, destination_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNkpgGuwMwEL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define the paths to your data\n",
        "data_dir = \"/content/AudioWithCategorisedWAV\"\n",
        "class_folders = os.listdir(data_dir)\n",
        "num_mfcc = 13  # Number of MFCC coefficients\n",
        "num_frames = 41  # Number of time frames for each MFCC\n",
        "\n",
        "# Initialize empty lists to store data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each class folder\n",
        "for i, class_folder in enumerate(class_folders):\n",
        "    class_path = os.path.join(data_dir, class_folder)\n",
        "\n",
        "    # Loop through audio files in the class folder\n",
        "    for audio_file in os.listdir(class_path):\n",
        "        audio_path = os.path.join(class_path, audio_file)\n",
        "\n",
        "        # Extract MFCC features from the audio file\n",
        "        audio, sr = librosa.load(audio_path, sr=None)\n",
        "        mfccs = librosa.feature.mfcc(y =audio, sr=sr, n_mfcc=num_mfcc)\n",
        "\n",
        "        # Make sure all MFCC feature matrices have the same shape\n",
        "        if mfccs.shape[1] < num_frames:\n",
        "            pad_width = num_frames - mfccs.shape[1]\n",
        "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        elif mfccs.shape[1] > num_frames:\n",
        "            mfccs = mfccs[:, :num_frames]\n",
        "\n",
        "        data.append(mfccs)\n",
        "        labels.append(i)  # Assign a class label to each MFCC feature\n",
        "\n",
        "# Convert data and labels to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=len(class_folders))\n",
        "y_test = to_categorical(y_test, num_classes=len(class_folders))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_A8zNqowRFR",
        "outputId": "36cb5fd9-7987-4338-c75e-a2e9ef2e5b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 43.32%\n",
            "Model Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data1=[]\n",
        "for x in data:\n",
        "  data1.append(np.ravel(x))\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X = np.array(data1)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a RandomForestClassifier\n",
        "audio_classifier = RandomForestClassifier(n_estimators=500, max_depth= 50, random_state=42)\n",
        "audio_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = audio_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "y_pred = audio_classifier.predict(X_train)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Brgx6ZEcI2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Define the paths to your data\n",
        "data_dir = \"/content/AudioWithCategorisedWAV\"\n",
        "class_folders = os.listdir(data_dir)\n",
        "num_mfcc = 13  # Number of MFCC coefficients\n",
        "\n",
        "# Initialize empty lists to store data and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Loop through each class folder\n",
        "for i, class_folder in enumerate(class_folders):\n",
        "    class_path = os.path.join(data_dir, class_folder)\n",
        "\n",
        "    # Loop through audio files in the class folder\n",
        "    for audio_file in os.listdir(class_path):\n",
        "        audio_path = os.path.join(class_path, audio_file)\n",
        "\n",
        "        # Extract MFCC features from the audio file\n",
        "        audio, sr = librosa.load(audio_path, sr=None)\n",
        "        mfccs = librosa.feature.mfcc(y =audio, sr=sr, n_mfcc=num_mfcc)\n",
        "        # Normalize MFCCs\n",
        "        mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
        "\n",
        "        # Data Augmentation: Random pitch shift\n",
        "        pitch_shift_steps = np.random.randint(low=-5, high=5)\n",
        "        y_pitch_shifted = librosa.effects.pitch_shift(audio,sr=sr, n_steps=pitch_shift_steps)\n",
        "        augmented_mfccs = librosa.feature.mfcc(y=y_pitch_shifted, sr=sr, n_mfcc=13)\n",
        "\n",
        "        # Make sure all MFCC feature matrices have the same shape\n",
        "        if augmented_mfccs.shape[1] < num_frames:\n",
        "            pad_width = num_frames - augmented_mfccs.shape[1]\n",
        "            augmented_mfccs = np.pad(augmented_mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        elif augmented_mfccs.shape[1] > num_frames:\n",
        "            augmented_mfccs = augmented_mfccs[:, :num_frames]\n",
        "        features.append(np.expand_dims(augmented_mfccs, axis=-1))\n",
        "        labels.append(i)  # Assign a class label to each MFCC feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5YC7-suAVfb",
        "outputId": "abb4451e-1cd2-461a-ae31-501e3759c8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "149/149 [==============================] - 11s 61ms/step - loss: 1.8320 - accuracy: 0.2866 - val_loss: 1.5682 - val_accuracy: 0.3526\n",
            "Epoch 2/50\n",
            "149/149 [==============================] - 8s 54ms/step - loss: 1.5789 - accuracy: 0.3381 - val_loss: 1.5024 - val_accuracy: 0.3720\n",
            "Epoch 3/50\n",
            "149/149 [==============================] - 8s 57ms/step - loss: 1.5567 - accuracy: 0.3419 - val_loss: 1.4972 - val_accuracy: 0.3778\n",
            "Epoch 4/50\n",
            "149/149 [==============================] - 8s 56ms/step - loss: 1.5152 - accuracy: 0.3587 - val_loss: 1.4826 - val_accuracy: 0.3820\n",
            "Epoch 5/50\n",
            "149/149 [==============================] - 8s 54ms/step - loss: 1.5083 - accuracy: 0.3650 - val_loss: 1.4857 - val_accuracy: 0.3829\n",
            "Epoch 6/50\n",
            "149/149 [==============================] - 9s 59ms/step - loss: 1.4815 - accuracy: 0.3772 - val_loss: 1.5182 - val_accuracy: 0.3434\n",
            "Epoch 7/50\n",
            "149/149 [==============================] - 8s 52ms/step - loss: 1.4704 - accuracy: 0.3923 - val_loss: 1.4999 - val_accuracy: 0.3703\n",
            "Epoch 8/50\n",
            "149/149 [==============================] - 9s 61ms/step - loss: 1.4507 - accuracy: 0.4013 - val_loss: 1.4747 - val_accuracy: 0.3736\n",
            "Epoch 9/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 1.4400 - accuracy: 0.3912 - val_loss: 1.4492 - val_accuracy: 0.3971\n",
            "Epoch 10/50\n",
            "149/149 [==============================] - 10s 66ms/step - loss: 1.4241 - accuracy: 0.4122 - val_loss: 1.5037 - val_accuracy: 0.3770\n",
            "Epoch 11/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 1.4117 - accuracy: 0.4135 - val_loss: 1.4743 - val_accuracy: 0.3862\n",
            "Epoch 12/50\n",
            "149/149 [==============================] - 10s 67ms/step - loss: 1.3777 - accuracy: 0.4299 - val_loss: 1.4558 - val_accuracy: 0.3846\n",
            "Epoch 13/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 1.3527 - accuracy: 0.4450 - val_loss: 1.5308 - val_accuracy: 0.3946\n",
            "Epoch 14/50\n",
            "149/149 [==============================] - 13s 89ms/step - loss: 1.3433 - accuracy: 0.4441 - val_loss: 1.4666 - val_accuracy: 0.3938\n",
            "Epoch 15/50\n",
            "149/149 [==============================] - 15s 98ms/step - loss: 1.3155 - accuracy: 0.4538 - val_loss: 1.4846 - val_accuracy: 0.4039\n",
            "Epoch 16/50\n",
            "149/149 [==============================] - 9s 58ms/step - loss: 1.3042 - accuracy: 0.4614 - val_loss: 1.4602 - val_accuracy: 0.3963\n",
            "Epoch 17/50\n",
            "149/149 [==============================] - 10s 65ms/step - loss: 1.2706 - accuracy: 0.4664 - val_loss: 1.5448 - val_accuracy: 0.3963\n",
            "Epoch 18/50\n",
            "149/149 [==============================] - 7s 47ms/step - loss: 1.2430 - accuracy: 0.4784 - val_loss: 1.5248 - val_accuracy: 0.3753\n",
            "Epoch 19/50\n",
            "149/149 [==============================] - 9s 63ms/step - loss: 1.2238 - accuracy: 0.4954 - val_loss: 1.5434 - val_accuracy: 0.4123\n",
            "Epoch 20/50\n",
            "149/149 [==============================] - 7s 49ms/step - loss: 1.2020 - accuracy: 0.5031 - val_loss: 1.6457 - val_accuracy: 0.3753\n",
            "Epoch 21/50\n",
            "149/149 [==============================] - 9s 61ms/step - loss: 1.1735 - accuracy: 0.5254 - val_loss: 1.5033 - val_accuracy: 0.3829\n",
            "Epoch 22/50\n",
            "149/149 [==============================] - 8s 52ms/step - loss: 1.1356 - accuracy: 0.5309 - val_loss: 1.5270 - val_accuracy: 0.3829\n",
            "Epoch 23/50\n",
            "149/149 [==============================] - 9s 58ms/step - loss: 1.1092 - accuracy: 0.5466 - val_loss: 1.5450 - val_accuracy: 0.3871\n",
            "Epoch 24/50\n",
            "149/149 [==============================] - 9s 62ms/step - loss: 1.0599 - accuracy: 0.5619 - val_loss: 1.6408 - val_accuracy: 0.3829\n",
            "Epoch 25/50\n",
            "149/149 [==============================] - 9s 60ms/step - loss: 1.0411 - accuracy: 0.5691 - val_loss: 1.5862 - val_accuracy: 0.3913\n",
            "Epoch 26/50\n",
            "149/149 [==============================] - 8s 53ms/step - loss: 1.0197 - accuracy: 0.5884 - val_loss: 1.9019 - val_accuracy: 0.3887\n",
            "Epoch 27/50\n",
            "149/149 [==============================] - 8s 57ms/step - loss: 0.9906 - accuracy: 0.5964 - val_loss: 1.6237 - val_accuracy: 0.4114\n",
            "Epoch 28/50\n",
            "149/149 [==============================] - 8s 56ms/step - loss: 0.9533 - accuracy: 0.6128 - val_loss: 1.7477 - val_accuracy: 0.4148\n",
            "Epoch 29/50\n",
            "149/149 [==============================] - 8s 55ms/step - loss: 0.9083 - accuracy: 0.6308 - val_loss: 1.8167 - val_accuracy: 0.4089\n",
            "Epoch 30/50\n",
            "149/149 [==============================] - 9s 58ms/step - loss: 0.8775 - accuracy: 0.6520 - val_loss: 2.1562 - val_accuracy: 0.3946\n",
            "Epoch 31/50\n",
            "149/149 [==============================] - 8s 53ms/step - loss: 0.8602 - accuracy: 0.6596 - val_loss: 1.8090 - val_accuracy: 0.4139\n",
            "Epoch 32/50\n",
            "149/149 [==============================] - 9s 60ms/step - loss: 0.8117 - accuracy: 0.6756 - val_loss: 1.9319 - val_accuracy: 0.4039\n",
            "Epoch 33/50\n",
            "149/149 [==============================] - 8s 51ms/step - loss: 0.7915 - accuracy: 0.6816 - val_loss: 1.9455 - val_accuracy: 0.3971\n",
            "Epoch 34/50\n",
            "149/149 [==============================] - 9s 62ms/step - loss: 0.7681 - accuracy: 0.6926 - val_loss: 1.9726 - val_accuracy: 0.3862\n",
            "Epoch 35/50\n",
            "149/149 [==============================] - 7s 45ms/step - loss: 0.7347 - accuracy: 0.7148 - val_loss: 2.0868 - val_accuracy: 0.4114\n",
            "Epoch 36/50\n",
            "149/149 [==============================] - 10s 68ms/step - loss: 0.6903 - accuracy: 0.7178 - val_loss: 2.0740 - val_accuracy: 0.4089\n",
            "Epoch 37/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 0.6586 - accuracy: 0.7400 - val_loss: 2.1405 - val_accuracy: 0.3963\n",
            "Epoch 38/50\n",
            "149/149 [==============================] - 10s 68ms/step - loss: 0.6125 - accuracy: 0.7549 - val_loss: 2.2931 - val_accuracy: 0.3946\n",
            "Epoch 39/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 0.6456 - accuracy: 0.7453 - val_loss: 2.0633 - val_accuracy: 0.4039\n",
            "Epoch 40/50\n",
            "149/149 [==============================] - 10s 69ms/step - loss: 0.5708 - accuracy: 0.7684 - val_loss: 2.3729 - val_accuracy: 0.3980\n",
            "Epoch 41/50\n",
            "149/149 [==============================] - 7s 46ms/step - loss: 0.5598 - accuracy: 0.7827 - val_loss: 2.4022 - val_accuracy: 0.3846\n",
            "Epoch 42/50\n",
            "149/149 [==============================] - 10s 67ms/step - loss: 0.5495 - accuracy: 0.7822 - val_loss: 2.4520 - val_accuracy: 0.3862\n",
            "Epoch 43/50\n",
            "149/149 [==============================] - 7s 45ms/step - loss: 0.5353 - accuracy: 0.7866 - val_loss: 2.4070 - val_accuracy: 0.3820\n",
            "Epoch 44/50\n",
            "149/149 [==============================] - 9s 63ms/step - loss: 0.4957 - accuracy: 0.8051 - val_loss: 2.5585 - val_accuracy: 0.3896\n",
            "Epoch 45/50\n",
            "149/149 [==============================] - 7s 49ms/step - loss: 0.4719 - accuracy: 0.8106 - val_loss: 2.7684 - val_accuracy: 0.3988\n",
            "Epoch 46/50\n",
            "149/149 [==============================] - 9s 61ms/step - loss: 0.4648 - accuracy: 0.8146 - val_loss: 2.8186 - val_accuracy: 0.4005\n",
            "Epoch 47/50\n",
            "149/149 [==============================] - 10s 66ms/step - loss: 0.4463 - accuracy: 0.8255 - val_loss: 2.5839 - val_accuracy: 0.3837\n",
            "Epoch 48/50\n",
            "149/149 [==============================] - 12s 80ms/step - loss: 0.4153 - accuracy: 0.8383 - val_loss: 2.5504 - val_accuracy: 0.3887\n",
            "Epoch 49/50\n",
            "149/149 [==============================] - 7s 45ms/step - loss: 0.4152 - accuracy: 0.8421 - val_loss: 2.7044 - val_accuracy: 0.3862\n",
            "Epoch 50/50\n",
            "149/149 [==============================] - 11s 76ms/step - loss: 0.4247 - accuracy: 0.8328 - val_loss: 3.0700 - val_accuracy: 0.3946\n",
            "Model Accuracy: 38.35%\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X = np.stack(features, axis=0)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], X.shape[3])),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(len(np.unique(y_encoded)), activation='softmax')\n",
        "])\n",
        "\n",
        "optimiser = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_QlbJLUYhVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqkNe_34VE2B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to extract audio features using librosa\n",
        "def extract_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    audio, sample_rate = librosa.load(file_path)\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, mel))\n",
        "    return result\n",
        "\n",
        "# Function to load audio data and labels\n",
        "def load_data(data_path):\n",
        "    features, labels = [], []\n",
        "    for folder in os.listdir(data_path):\n",
        "        label = folder\n",
        "        for file_name in os.listdir(os.path.join(data_path, folder)):\n",
        "            file_path = os.path.join(data_path, folder, file_name)\n",
        "            feature = extract_features(file_path)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load data and preprocess\n",
        "data_path = \"/content/AudioWithCategorisedWAV\"\n",
        "features, labels = load_data(data_path)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Conv1D(64, kernel_size=3, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(len(np.unique(encoded_labels)), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "#model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "#test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "#print(f'Test accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doLoW0Ek-829"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.GRU(64, return_sequences=True))\n",
        "model.add(layers.GRU(64))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(len(np.unique(encoded_labels)), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcUYalXKB70u"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(layers.Conv1D(256, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Recurrent layers\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp10z2Vn-9rV",
        "outputId": "57107451-3a2f-4ba0-ac23-b635c2b50e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "80/80 [==============================] - 71s 877ms/step - loss: 1.5932 - accuracy: 0.3165 - val_loss: 1.6859 - val_accuracy: 0.2609\n",
            "Epoch 2/50\n",
            "80/80 [==============================] - 66s 832ms/step - loss: 1.4961 - accuracy: 0.3877 - val_loss: 1.8440 - val_accuracy: 0.2078\n",
            "Epoch 3/50\n",
            "80/80 [==============================] - 63s 786ms/step - loss: 1.4693 - accuracy: 0.3990 - val_loss: 2.8142 - val_accuracy: 0.1703\n",
            "Epoch 4/50\n",
            "80/80 [==============================] - 63s 787ms/step - loss: 1.4606 - accuracy: 0.3931 - val_loss: 2.0740 - val_accuracy: 0.1984\n",
            "Epoch 5/50\n",
            "80/80 [==============================] - 63s 785ms/step - loss: 1.4255 - accuracy: 0.4189 - val_loss: 1.5473 - val_accuracy: 0.3500\n",
            "Epoch 6/50\n",
            "80/80 [==============================] - 69s 867ms/step - loss: 1.4096 - accuracy: 0.4248 - val_loss: 1.4697 - val_accuracy: 0.4125\n",
            "Epoch 7/50\n",
            "80/80 [==============================] - 69s 865ms/step - loss: 1.3906 - accuracy: 0.4306 - val_loss: 1.4849 - val_accuracy: 0.3547\n",
            "Epoch 8/50\n",
            "80/80 [==============================] - 66s 830ms/step - loss: 1.3552 - accuracy: 0.4529 - val_loss: 1.5068 - val_accuracy: 0.3766\n",
            "Epoch 9/50\n",
            "80/80 [==============================] - 65s 812ms/step - loss: 1.3651 - accuracy: 0.4525 - val_loss: 1.5858 - val_accuracy: 0.3422\n",
            "Epoch 10/50\n",
            "80/80 [==============================] - 67s 837ms/step - loss: 1.3280 - accuracy: 0.4588 - val_loss: 1.5138 - val_accuracy: 0.3609\n",
            "Epoch 11/50\n",
            "80/80 [==============================] - 71s 896ms/step - loss: 1.3041 - accuracy: 0.4725 - val_loss: 4.1668 - val_accuracy: 0.1719\n",
            "Epoch 12/50\n",
            "80/80 [==============================] - 66s 825ms/step - loss: 1.2750 - accuracy: 0.4842 - val_loss: 1.6064 - val_accuracy: 0.3812\n",
            "Epoch 13/50\n",
            "80/80 [==============================] - 69s 867ms/step - loss: 1.2498 - accuracy: 0.4990 - val_loss: 1.6784 - val_accuracy: 0.3750\n",
            "Epoch 14/50\n",
            "80/80 [==============================] - 65s 816ms/step - loss: 1.2240 - accuracy: 0.5010 - val_loss: 1.4684 - val_accuracy: 0.4078\n",
            "Epoch 15/50\n",
            "80/80 [==============================] - 66s 824ms/step - loss: 1.2014 - accuracy: 0.5190 - val_loss: 1.4321 - val_accuracy: 0.4375\n",
            "Epoch 16/50\n",
            "80/80 [==============================] - 71s 885ms/step - loss: 1.1728 - accuracy: 0.5318 - val_loss: 1.5040 - val_accuracy: 0.4437\n",
            "Epoch 17/50\n",
            "80/80 [==============================] - 68s 851ms/step - loss: 1.1362 - accuracy: 0.5530 - val_loss: 1.7742 - val_accuracy: 0.3906\n",
            "Epoch 18/50\n",
            "80/80 [==============================] - 66s 827ms/step - loss: 1.0988 - accuracy: 0.5576 - val_loss: 1.4581 - val_accuracy: 0.4391\n",
            "Epoch 19/50\n",
            "80/80 [==============================] - 72s 897ms/step - loss: 1.0757 - accuracy: 0.5791 - val_loss: 2.1808 - val_accuracy: 0.3328\n",
            "Epoch 20/50\n",
            "80/80 [==============================] - 68s 850ms/step - loss: 0.9935 - accuracy: 0.6139 - val_loss: 2.8111 - val_accuracy: 0.3047\n",
            "Epoch 21/50\n",
            "80/80 [==============================] - 72s 905ms/step - loss: 0.9796 - accuracy: 0.6233 - val_loss: 1.6994 - val_accuracy: 0.3938\n",
            "Epoch 22/50\n",
            "80/80 [==============================] - 66s 824ms/step - loss: 0.9373 - accuracy: 0.6448 - val_loss: 1.8539 - val_accuracy: 0.3906\n",
            "Epoch 23/50\n",
            "80/80 [==============================] - 66s 823ms/step - loss: 0.8953 - accuracy: 0.6549 - val_loss: 2.5170 - val_accuracy: 0.3453\n",
            "Epoch 24/50\n",
            "80/80 [==============================] - 72s 898ms/step - loss: 0.8537 - accuracy: 0.6631 - val_loss: 1.6966 - val_accuracy: 0.4094\n",
            "Epoch 25/50\n",
            "80/80 [==============================] - 66s 822ms/step - loss: 0.7770 - accuracy: 0.7003 - val_loss: 1.7379 - val_accuracy: 0.4688\n",
            "Epoch 26/50\n",
            "80/80 [==============================] - 69s 869ms/step - loss: 0.7419 - accuracy: 0.7210 - val_loss: 1.6767 - val_accuracy: 0.4469\n",
            "Epoch 27/50\n",
            "80/80 [==============================] - 66s 824ms/step - loss: 0.6565 - accuracy: 0.7456 - val_loss: 2.4734 - val_accuracy: 0.4031\n",
            "Epoch 28/50\n",
            "80/80 [==============================] - 71s 886ms/step - loss: 0.6237 - accuracy: 0.7714 - val_loss: 1.9989 - val_accuracy: 0.4563\n",
            "Epoch 29/50\n",
            "80/80 [==============================] - 64s 798ms/step - loss: 0.5777 - accuracy: 0.7788 - val_loss: 2.1442 - val_accuracy: 0.4156\n",
            "Epoch 30/50\n",
            "80/80 [==============================] - 72s 896ms/step - loss: 0.4914 - accuracy: 0.8140 - val_loss: 3.0342 - val_accuracy: 0.3625\n",
            "Epoch 31/50\n",
            "80/80 [==============================] - 71s 890ms/step - loss: 0.4726 - accuracy: 0.8312 - val_loss: 2.5508 - val_accuracy: 0.4172\n",
            "Epoch 32/50\n",
            "80/80 [==============================] - 67s 837ms/step - loss: 0.4085 - accuracy: 0.8503 - val_loss: 2.5844 - val_accuracy: 0.4359\n",
            "Epoch 33/50\n",
            "80/80 [==============================] - 65s 814ms/step - loss: 0.4029 - accuracy: 0.8562 - val_loss: 3.1721 - val_accuracy: 0.4000\n",
            "Epoch 34/50\n",
            "80/80 [==============================] - 67s 839ms/step - loss: 0.3477 - accuracy: 0.8753 - val_loss: 2.6433 - val_accuracy: 0.4109\n",
            "Epoch 35/50\n",
            "80/80 [==============================] - 68s 850ms/step - loss: 0.3372 - accuracy: 0.8750 - val_loss: 2.9665 - val_accuracy: 0.4422\n",
            "Epoch 36/50\n",
            "80/80 [==============================] - 65s 818ms/step - loss: 0.3174 - accuracy: 0.8859 - val_loss: 2.6381 - val_accuracy: 0.4359\n",
            "Epoch 37/50\n",
            "80/80 [==============================] - 76s 947ms/step - loss: 0.2586 - accuracy: 0.9082 - val_loss: 3.5588 - val_accuracy: 0.3750\n",
            "Epoch 38/50\n",
            "80/80 [==============================] - 64s 806ms/step - loss: 0.2187 - accuracy: 0.9261 - val_loss: 3.1790 - val_accuracy: 0.4156\n",
            "Epoch 39/50\n",
            "80/80 [==============================] - 66s 824ms/step - loss: 0.2216 - accuracy: 0.9234 - val_loss: 2.9919 - val_accuracy: 0.4469\n",
            "Epoch 40/50\n",
            "80/80 [==============================] - 64s 798ms/step - loss: 0.2120 - accuracy: 0.9277 - val_loss: 3.3849 - val_accuracy: 0.4219\n",
            "Epoch 41/50\n",
            "80/80 [==============================] - 65s 813ms/step - loss: 0.2432 - accuracy: 0.9187 - val_loss: 2.9932 - val_accuracy: 0.4313\n",
            "Epoch 42/50\n",
            "80/80 [==============================] - 64s 807ms/step - loss: 0.1722 - accuracy: 0.9328 - val_loss: 3.7953 - val_accuracy: 0.3906\n",
            "Epoch 43/50\n",
            "80/80 [==============================] - 65s 815ms/step - loss: 0.1394 - accuracy: 0.9504 - val_loss: 3.5243 - val_accuracy: 0.4328\n",
            "Epoch 44/50\n",
            "80/80 [==============================] - 66s 826ms/step - loss: 0.1414 - accuracy: 0.9508 - val_loss: 3.3384 - val_accuracy: 0.4234\n",
            "Epoch 45/50\n",
            "80/80 [==============================] - 69s 862ms/step - loss: 0.1424 - accuracy: 0.9527 - val_loss: 3.7656 - val_accuracy: 0.3812\n",
            "Epoch 46/50\n",
            "80/80 [==============================] - 63s 792ms/step - loss: 0.1775 - accuracy: 0.9371 - val_loss: 3.5428 - val_accuracy: 0.4172\n",
            "Epoch 47/50\n",
            "80/80 [==============================] - 64s 807ms/step - loss: 0.1385 - accuracy: 0.9453 - val_loss: 3.9026 - val_accuracy: 0.4125\n",
            "Epoch 48/50\n",
            "80/80 [==============================] - 63s 785ms/step - loss: 0.1689 - accuracy: 0.9433 - val_loss: 4.0387 - val_accuracy: 0.4016\n",
            "Epoch 49/50\n",
            "80/80 [==============================] - 69s 867ms/step - loss: 0.1422 - accuracy: 0.9461 - val_loss: 3.5923 - val_accuracy: 0.4344\n",
            "Epoch 50/50\n",
            "80/80 [==============================] - 63s 795ms/step - loss: 0.1203 - accuracy: 0.9597 - val_loss: 3.3102 - val_accuracy: 0.4422\n",
            "20/20 [==============================] - 6s 316ms/step - loss: 3.3102 - accuracy: 0.4422\n",
            "Test accuracy: 0.44218748807907104\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_labels = np.unique(labels)\n",
        "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "Y = np.array([class_indices[label] for label in labels])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight ='balanced',classes = np.unique(Y),y= Y)\n",
        "\n",
        "# Convert class weights to a dictionary for class_weight parameter in model.fit\n",
        "class_weights_dict = {class_index: weight for class_index, weight in zip(np.unique(Y), class_weights)}\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weights_dict)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-tnjqxjUdED"
      },
      "outputs": [],
      "source": [
        "# Convert lists to NumPy arrays\n",
        "X = np.stack(features, axis=0)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a simple model for classification\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(np.unique(y_encoded)), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDbVSODMDfUL",
        "outputId": "159a2513-4e3e-48f8-8216-f367b8e228d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2319"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hUIgZcBV4Kk"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(num_mfcc, num_frames, 1)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding = 'same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding = 'same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4rTeSp8UNBR",
        "outputId": "2ec823db-f64e-4c93-add7-1514dcae86a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.0585 - accuracy: 0.9892 - val_loss: 3.7565 - val_accuracy: 0.2953\n",
            "Epoch 2/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0597 - accuracy: 0.9903 - val_loss: 4.0677 - val_accuracy: 0.2522\n",
            "Epoch 3/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0815 - accuracy: 0.9844 - val_loss: 4.8646 - val_accuracy: 0.2672\n",
            "Epoch 4/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0893 - accuracy: 0.9833 - val_loss: 3.8512 - val_accuracy: 0.2802\n",
            "Epoch 5/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0613 - accuracy: 0.9903 - val_loss: 4.0606 - val_accuracy: 0.2672\n",
            "Epoch 6/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.0747 - accuracy: 0.9811 - val_loss: 3.9983 - val_accuracy: 0.2888\n",
            "Epoch 7/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.1362 - accuracy: 0.9650 - val_loss: 4.4263 - val_accuracy: 0.2845\n",
            "Epoch 8/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.1129 - accuracy: 0.9725 - val_loss: 4.0027 - val_accuracy: 0.2974\n",
            "Epoch 9/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.0890 - accuracy: 0.9779 - val_loss: 4.0371 - val_accuracy: 0.2780\n",
            "Epoch 10/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0974 - accuracy: 0.9779 - val_loss: 4.1690 - val_accuracy: 0.2672\n",
            "Epoch 11/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0768 - accuracy: 0.9876 - val_loss: 4.2175 - val_accuracy: 0.2716\n",
            "Epoch 12/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0691 - accuracy: 0.9849 - val_loss: 4.0537 - val_accuracy: 0.3125\n",
            "Epoch 13/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0609 - accuracy: 0.9844 - val_loss: 3.9734 - val_accuracy: 0.2888\n",
            "Epoch 14/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0831 - accuracy: 0.9838 - val_loss: 4.1578 - val_accuracy: 0.2759\n",
            "Epoch 15/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0667 - accuracy: 0.9871 - val_loss: 4.1598 - val_accuracy: 0.2543\n",
            "Epoch 16/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0565 - accuracy: 0.9898 - val_loss: 4.0146 - val_accuracy: 0.2694\n",
            "Epoch 17/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0837 - accuracy: 0.9881 - val_loss: 4.3714 - val_accuracy: 0.3103\n",
            "Epoch 18/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0557 - accuracy: 0.9865 - val_loss: 4.4957 - val_accuracy: 0.2349\n",
            "Epoch 19/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0702 - accuracy: 0.9919 - val_loss: 4.2658 - val_accuracy: 0.2478\n",
            "Epoch 20/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0814 - accuracy: 0.9822 - val_loss: 4.2924 - val_accuracy: 0.2974\n",
            "Epoch 21/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0729 - accuracy: 0.9844 - val_loss: 4.1045 - val_accuracy: 0.2866\n",
            "Epoch 22/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0674 - accuracy: 0.9881 - val_loss: 4.1970 - val_accuracy: 0.2629\n",
            "Epoch 23/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0774 - accuracy: 0.9833 - val_loss: 4.3797 - val_accuracy: 0.2737\n",
            "Epoch 24/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0759 - accuracy: 0.9822 - val_loss: 4.2605 - val_accuracy: 0.2586\n",
            "Epoch 25/30\n",
            "93/93 [==============================] - 1s 6ms/step - loss: 0.0765 - accuracy: 0.9822 - val_loss: 4.5359 - val_accuracy: 0.2392\n",
            "Epoch 26/30\n",
            "93/93 [==============================] - 1s 7ms/step - loss: 0.0918 - accuracy: 0.9801 - val_loss: 4.7059 - val_accuracy: 0.2349\n",
            "Epoch 27/30\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0632 - accuracy: 0.9838 - val_loss: 4.2791 - val_accuracy: 0.2845\n",
            "Epoch 28/30\n",
            "93/93 [==============================] - 1s 9ms/step - loss: 0.0793 - accuracy: 0.9844 - val_loss: 4.2138 - val_accuracy: 0.2866\n",
            "Epoch 29/30\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0670 - accuracy: 0.9844 - val_loss: 5.3568 - val_accuracy: 0.2500\n",
            "Epoch 30/30\n",
            "93/93 [==============================] - 1s 8ms/step - loss: 0.0507 - accuracy: 0.9908 - val_loss: 4.2989 - val_accuracy: 0.2931\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x785b3d126d10>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRDOw-LUUV8n",
        "outputId": "3c2d2942-3710-4aaf-90f4-e08572902d03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 0s 6ms/step - loss: 4.2989 - accuracy: 0.2931\n",
            "Test accuracy: 29.31%\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features_extractor = model"
      ],
      "metadata": {
        "id": "kVlc6TGeCPQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8oXZi2sIb3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "722ef1cd-c058-4c4e-bab3-1c504390c5d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-adadc42b23ff>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Load the video emotion classification model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoEmotionClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video_emotion_classification_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Load the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m    478\u001b[0m             )\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;34m\"Unable to load weights saved in HDF5 format into a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;34m\"subclassed Model which has not created its variables yet. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class VideoEmotionClassificationModel(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VideoEmotionClassificationModel, self).__init__()\n",
        "\n",
        "        # Audio features extraction layer\n",
        "        self.audio_features_extractor = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
        "            tf.keras.layers.MaxPooling1D(2),\n",
        "            tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "            tf.keras.layers.MaxPooling1D(2),\n",
        "            tf.keras.layers.Flatten(),\n",
        "        ])\n",
        "\n",
        "        # Video features extraction layer\n",
        "        self.video_features_extractor = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "            tf.keras.layers.Flatten(),\n",
        "        ])\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion_layer = tf.keras.layers.Dense(128, activation='relu')\n",
        "\n",
        "        # Classification layer\n",
        "        self.classification_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Extract audio and video features\n",
        "        audio_features = self.audio_features_extractor(inputs['audio'])\n",
        "        video_features = self.video_features_extractor(inputs['video'])\n",
        "\n",
        "        # Fuse audio and video features\n",
        "        fused_features = tf.concat([audio_features, video_features], axis=1)\n",
        "        fused_features = self.fusion_layer(fused_features)\n",
        "\n",
        "        # Classify the fused features\n",
        "        predictions = self.classification_layer(fused_features)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load the video emotion classification model\n",
        "model = VideoEmotionClassificationModel(num_classes=6)\n",
        "model.load_weights('video_emotion_classification_model.h5')\n",
        "\n",
        "# Load the video\n",
        "video = tf.io.read_file('video.mp4')\n",
        "\n",
        "# Decode the video\n",
        "video = tf.io.decode_video(video)\n",
        "\n",
        "# Extract audio and video features from the video\n",
        "audio_features = tf.reshape(video['audio'], (1, -1, 1))\n",
        "video_features = tf.reshape(video['video'], (1, 224, 224, 3))\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model({'audio': audio_features, 'video': video_features})\n",
        "\n",
        "# Get the most likely emotion\n",
        "predicted_emotion = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "\n",
        "# Print the predicted emotion\n",
        "print('Predicted emotion:', predicted_emotion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHHTraDCLmYS"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load the VGG19 model without the fully connected layers\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add your own fully connected layers for emotion classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_emotions, activation='softmax')(x)  # Add your number of emotion classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load the pre-trained weights if available\n",
        "# model.load_weights('your_weights.h5')\n",
        "\n",
        "# Load and process the video frames for emotion classification\n",
        "video_path = 'path_to_your_video.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Preprocess the frame for VGG19\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = image.img_to_array(frame)\n",
        "    frame = np.expand_dims(frame, axis=0)\n",
        "    frame = preprocess_input(frame)\n",
        "\n",
        "    # Make predictions\n",
        "    emotion_predictions = model.predict(frame)\n",
        "\n",
        "    # Aggregate or process the predictions here for your specific needs\n",
        "\n",
        "# Release the video capture and close any windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Model"
      ],
      "metadata": {
        "id": "Twp8bXCSkwK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNhBVi3zaREl"
      },
      "outputs": [],
      "source": [
        "def preprocess_frame(frame):\n",
        "    # Preprocess the frame: resize, normalize, etc.\n",
        "    processed_frame = cv2.resize(frame, (224, 224))\n",
        "    processed_frame = processed_frame / 255.0  # Normalize pixel values\n",
        "    return processed_frame\n",
        "\n",
        "def read_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        processed_frame = preprocess_frame(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return processed_frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv00h-SCQX7U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import shutil\n",
        "sourcepath = '/content/Videos'\n",
        "files = os.listdir(sourcepath)\n",
        "xdata = []\n",
        "ylabels = []\n",
        "\n",
        "for file in files:\n",
        "    source_file = os.path.join(sourcepath, file)\n",
        "    xdata.append(read_video(source_file))\n",
        "    if 'ANG' in file:\n",
        "      ylabels.append('Ang')\n",
        "    elif 'DIS' in file:\n",
        "      ylabels.append('DIS')\n",
        "    elif 'FEA' in file:\n",
        "      ylabels.append('FEA')\n",
        "    elif 'HAP' in file:\n",
        "      ylabels.append('HAP')\n",
        "    elif 'NEU' in file:\n",
        "      ylabels.append('NEU')\n",
        "    elif 'SAD' in file:\n",
        "      ylabels.append('SAD')\n",
        "\n",
        "xdata1 = np.array(xdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c2gYnuDL_Sv"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "# Get unique class labels\n",
        "class_labels = np.unique(ylabels)\n",
        "\n",
        "# Load the VGG19 model without the fully connected layers\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add your own fully connected layers for emotion classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(6, activation='softmax')(x)  # Add your number of emotion classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrl6zgZdh0zG",
        "outputId": "f7a41889-1c76-4df0-fc58-96a5086510da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(xdata1, ylabels, test_size=0.1, random_state=42)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse = False)\n",
        "y_train_onehot = onehot_encoder.fit_transform((np.array(y_train1)).reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H5Fn5S0Rdat"
      },
      "outputs": [],
      "source": [
        "class_labels = np.unique(y_train1)\n",
        "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "Y = np.array([class_indices[label] for label in y_train1])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight ='balanced',classes = np.unique(Y),y= Y)\n",
        "\n",
        "# Convert class weights to a dictionary for class_weight parameter in model.fit\n",
        "class_weights_dict = {class_index: weight for class_index, weight in zip(np.unique(Y), class_weights)}\n",
        "\n",
        "model.fit(x=X_train1,y=y_train_onehot,batch_size=16,epochs=20, class_weight=class_weights_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_onehot1 = onehot_encoder.transform((np.array(y_test1)).reshape(-1, 1))\n",
        "model.evaluate(X_test1,y_test_onehot1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl7RN8MuoXgy",
        "outputId": "8b75ba6c-0caf-420a-e1b3-df8e4e6ac22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 5s 109ms/step - loss: 0.9843 - accuracy: 0.6364\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9843055605888367, 0.6363636255264282]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GViz2NMbg7qW",
        "outputId": "fc1db177-053d-413a-8279-a4a761ddf61b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save(\"emotion_classification_Video_model.h5\")\n",
        "\n",
        "# Optionally, you can also save the weights only\n",
        "model.save_weights(\"emotion_classification_model_Video_weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iSGhWNJFMa0z"
      },
      "outputs": [],
      "source": [
        "video_path = '/content/Videos/1001_IEO__HI.flv'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "predicted_emotion = -1\n",
        "emotion_predictions = -1\n",
        "emotion_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    # Preprocess the frame for VGG19\n",
        "    frame = cv2.resize(frame, (224, 224))\n",
        "    frame = image.img_to_array(frame)\n",
        "    frame = np.expand_dims(frame, axis=0)\n",
        "    frame = preprocess_input(frame)\n",
        "\n",
        "    # Make predictions\n",
        "    emotion_predictions = model.predict(frame)\n",
        "    predicted_emotion = np.argmax(emotion_predictions)\n",
        "    emotion_counts[predicted_emotion] += 1\n",
        "\n",
        "    # Aggregate or process the predictions here for your specific needs\n",
        "\n",
        "#print(predicted_emotion)\n",
        "print(max(emotion_counts, key=emotion_counts.get))\n",
        "# Release the video capture\n",
        "cap.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdM31752CVqR"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install nltk\n",
        "#!pip install transformers\n",
        "#!pip install --upgrade protobuf\n",
        "#!pip install --upgrade tensorflow --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfiU4U2EP6Ei"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "\n",
        "import contractions\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4DMMNq-CIxI"
      },
      "outputs": [],
      "source": [
        "from transformers import TFRobertaModel, RobertaTokenizerFast\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tabulate import tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7miovCgChq8",
        "outputId": "c171e9fc-1f82-43d1-8827-f621e9b4916e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0vdvA0PCoU8"
      },
      "outputs": [],
      "source": [
        "df_clean = pd.read_csv('tweet_emotions.csv')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def expand_contractions(text):\n",
        "    '''\n",
        "    Function replaces abbreviations with full word versions\n",
        "    '''\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def clean_content(text):\n",
        "\n",
        "    text = expand_contractions(text)\n",
        "    # remove twitter handles\n",
        "    clean_text = re.sub(r'@\\w+\\s?', '', text)\n",
        "\n",
        "    # convert to lowercase\n",
        "    clean_text = clean_text.lower()\n",
        "\n",
        "    # remove links http:// or https://\n",
        "    clean_text = re.sub(r'https?:\\/\\/\\S+', '', clean_text)\n",
        "\n",
        "    # remove links beginning with www. and ending with .com\n",
        "    clean_text = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', clean_text)\n",
        "\n",
        "    # remove html reference characters\n",
        "    clean_text = re.sub(r'&[a-z]+;', '', clean_text)\n",
        "\n",
        "    # remove non-letter characters besides spaces \"/\", \";\" \"[\", \"]\" \"=\", \"#\"\n",
        "    clean_text = re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', clean_text)\n",
        "    clean_text = clean_text.split()\n",
        "\n",
        "    # remove stop words\n",
        "    clean_lst = []\n",
        "    for word in clean_text:\n",
        "      if word not in stop_words:\n",
        "        clean_lst.append(word)\n",
        "\n",
        "\n",
        "    lemmatized_words = []\n",
        "    for word in clean_lst:\n",
        "      lemmatized_word = WordNetLemmatizer().lemmatize(word)\n",
        "      lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "df_clean['content'] = df_clean['content'].apply(lambda x :  clean_content(x))\n",
        "\n",
        "# delete duplicates\n",
        "df_clean.drop_duplicates(subset='content', inplace=True)\n",
        "#df_clean.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# delete small sentence\n",
        "df_clean = df_clean.loc[df_clean['content'].apply(lambda x: len(x) >= 3)]\n",
        "\n",
        "# splitting into tokens, features of the structure of the text used in Twitter\n",
        "df_clean['content'] = df_clean['content'].apply(TweetTokenizer().tokenize)\n",
        "\n",
        "# remove punctuation marks\n",
        "PUNCUATION_LIST = list(string.punctuation)\n",
        "def remove_punctuation(word_list):\n",
        "    return [w for w in word_list if w not in PUNCUATION_LIST]\n",
        "df_clean['content'] = df_clean['content'].apply(remove_punctuation)\n",
        "df_clean['content'] = df_clean['content'].apply(lambda x: ' '.join(x))\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['happiness', 'enthusiasm', 'surprise','love','fun'], 'Happy')\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['boredom','sadness','Sad'], 'Sad')\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['anger'], 'Anger')\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['hate'], 'Disgust')\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['worry'], 'Fear')\n",
        "df_clean['sentiment'] = df_clean['sentiment'].replace(['relief', 'empty', 'neutral'], 'Neutral')\n",
        "X_train, X_test, y_train, y_test= train_test_split(df_clean['content'], df_clean['sentiment'], test_size=0.2, random_state=42)\n",
        "onehot_encoder = OneHotEncoder(sparse = False)\n",
        "y_train_onehot = onehot_encoder.fit_transform((np.array(df_clean['sentiment'])).reshape(-1, 1))\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the label encoder and transform the categories into numerical labels\n",
        "y_label_encoder = label_encoder.fit_transform(df_clean['sentiment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "7ksjLdWqC7fR",
        "outputId": "5e0fffc5-a0e6-43a6-8a09-3de51315a90a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8cce8697284f>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8cce8697284f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Forward pass through RoBERTa base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Extract the last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Load pre-trained RoBERTa model and tokenizer\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=6)  # 6 output units\n",
        "\n",
        "# Customize the architecture by adding new layers\n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self, roberta_model):\n",
        "        super(CustomRobertaModel, self).__init__()\n",
        "        self.roberta = roberta_model.roberta  # Extract the RoBERTa base model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(roberta_model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 6)  # 6 output units for 6 labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Forward pass through RoBERTa base model\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # Extract the last hidden state\n",
        "        last_hidden_state = outputs.last_hidden_state[:, 0, :]\n",
        "        # Forward pass through custom classifier\n",
        "        logits = self.classifier(last_hidden_state)\n",
        "        return logits\n",
        "\n",
        "# Create an instance of the custom model\n",
        "model = CustomRobertaModel(roberta_model)\n",
        "\n",
        "# Freeze pre-trained RoBERTa layers\n",
        "for param in model.roberta.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Example data (replace with your own dataset)\n",
        "texts = df_clean['content'].to_list()\n",
        "labels = y_label_encoder # Assuming 6 labels\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Custom dataset class for RoBERTa\n",
        "class MyRoBERTaDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MyRoBERTaDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = MyRoBERTaDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# Define optimizer for the task-specific layers\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(inputs, attention_mask=attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop (evaluate model performance on validation set)\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_batch in val_loader:\n",
        "            val_inputs = val_batch[\"input_ids\"]\n",
        "            val_attention_mask = val_batch[\"attention_mask\"]\n",
        "            val_labels = val_batch[\"labels\"]\n",
        "\n",
        "            val_logits = model(val_inputs, attention_mask=val_attention_mask)\n",
        "            val_loss = nn.CrossEntropyLoss()(val_logits, val_labels)\n",
        "            total_val_loss += val_loss.item()\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, avg_loss: {average_loss}, Val Loss: {average_val_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tyH_1lipT-C"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizerFast, TFRobertaModel\n",
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h0x4HWVu8Eq"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "\n",
        "ros = RandomOverSampler()\n",
        "x_train, y_train = ros.fit_resample(np.array(df_clean['content']).reshape(-1, 1), np.array(df_clean['sentiment']).reshape(-1, 1))\n",
        "train_os = pd.DataFrame(list(zip([x[0] for x in x_train], y_train)), columns = ['content', 'sentiment'])\n",
        "X_train = train_os['content'].values\n",
        "y_train = train_os['sentiment'].values\n",
        "\n",
        "X_test = test_df['content'].values\n",
        "y_test = test_df['sentiment'].values\n",
        "\n",
        "X_valid = valid_df['content'].values\n",
        "y_valid = valid_df['sentiment'].values\n",
        "\n",
        "y_train = OneHotEncoder().fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
        "y_valid = OneHotEncoder().fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
        "y_test = OneHotEncoder().fit_transform(np.array(y_test).reshape(-1, 1)).toarray()\n",
        "\n",
        "token_lens = []\n",
        "\n",
        "for txt in X_train:\n",
        "    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "max_length=np.max(token_lens)\n",
        "\n",
        "MAX_LEN=128\n",
        "\n",
        "def tokenize_roberta(data, max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "train_inputs, train_masks = tokenize_roberta(X_train, MAX_LEN)\n",
        "val_inputs, val_masks = tokenize_roberta(X_valid, MAX_LEN)\n",
        "test_inputs, test_masks = tokenize_roberta(X_test, MAX_LEN)\n",
        "\n",
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    inputs = Input(shape=(max_len,), dtype='int32')\n",
        "    masks = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "    bert_output = bert_model([inputs, masks])[1]\n",
        "\n",
        "    dense_1 = Dense(128, activation='relu')(bert_output)\n",
        "    dropout_1 = Dropout(0.5)(dense_1)\n",
        "\n",
        "    dense_2 = Dense(64, activation='relu')(dropout_1)\n",
        "    dropout_2 = Dropout(0.5)(dense_2)\n",
        "\n",
        "    output = Dense(6, activation='softmax')(dropout_2)\n",
        "\n",
        "    model = Model(inputs=[inputs, masks], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5, decay=1e-7),\n",
        "                  loss=CategoricalCrossentropy(),\n",
        "                  metrics=CategoricalAccuracy())\n",
        "    return model\n",
        "\n",
        "roberta_model = TFRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "model = create_model(roberta_model, MAX_LEN)\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_categorical_accuracy', patience=5, min_delta=0.01),\n",
        "             ModelCheckpoint(filepath='best_model.h5', monitor='val_categorical_accuracy', save_best_only=True)]\n",
        "\n",
        "history = model.fit(\n",
        "    [train_inputs, train_masks],\n",
        "    y_train,\n",
        "    validation_data=([val_inputs, val_masks], y_valid),\n",
        "    epochs=4,\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ros = RandomOverSampler()\n",
        "#x_train, y_train = ros.fit_resample(np.array(df_clean['content']).reshape(-1, 1), np.array(df_clean['sentiment']).reshape(-1, 1))\n",
        "#train_os = pd.DataFrame(list(zip([x[0] for x in x_train], y_train)), columns = ['content', 'sentiment'])\n",
        "X_train = df_clean['content'].values\n",
        "y_train = df_clean['sentiment'].values\n",
        "\n",
        "y_train = OneHotEncoder().fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
        "\n",
        "token_lens = []\n",
        "\n",
        "for txt in X_train:\n",
        "    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))\n",
        "max_length=np.max(token_lens)\n"
      ],
      "metadata": {
        "id": "gski1vdtUTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvf1zHGM_-jK",
        "outputId": "ddbd8c05-41f3-47f5-d459-ee864fbf130c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion were not used when initializing TFRobertaModel: ['classifier']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFRobertaModel, RobertaTokenizerFast\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from keras.metrics import CategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "tokenizer_roberta = RobertaTokenizerFast.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "\n",
        "MAX_LEN=128\n",
        "\n",
        "def tokenize_roberta(data, max_len=MAX_LEN) :\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for i in range(len(data)):\n",
        "        encoded = tokenizer_roberta.encode_plus(\n",
        "            data[i],\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "    return np.array(input_ids),np.array(attention_masks)\n",
        "\n",
        "def create_model(bert_model, max_len=MAX_LEN):\n",
        "    inputs = Input(shape=(max_len,), dtype='int32')\n",
        "    masks = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "    bert_output = bert_model([inputs, masks])[1]\n",
        "\n",
        "    dense_1 = Dense(128, activation='relu')(bert_output)\n",
        "    dropout_1 = Dropout(0.5)(dense_1)\n",
        "\n",
        "    dense_2 = Dense(64, activation='relu')(dropout_1)\n",
        "    dropout_2 = Dropout(0.5)(dense_2)\n",
        "\n",
        "    output = Dense(6, activation='softmax')(dropout_2)\n",
        "\n",
        "    model = Model(inputs=[inputs, masks], outputs=output)\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "                  loss=CategoricalCrossentropy(),\n",
        "                  metrics=CategoricalAccuracy())\n",
        "    return model\n",
        "\n",
        "roberta_model = TFRobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base-emotion')\n",
        "model = create_model(roberta_model, MAX_LEN)\n",
        "\n",
        "#callbacks = [EarlyStopping(monitor='val_categorical_accuracy', patience=5, min_delta=0.01), ModelCheckpoint(filepath='best_model.h5', monitor='val_categorical_accuracy', save_best_only=True)]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, train_masks = tokenize_roberta(X_train1, MAX_LEN)#tokenize_roberta(X_train, MAX_LEN)\n",
        "history = model.fit([train_inputs, train_masks],  OneHotEncoder().fit_transform(np.array(y_train1).reshape(-1, 1)).toarray(),  epochs=10,  batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeA1EpLQr2lj",
        "outputId": "617c562f-b70d-42a6-d686-fa5f803ba2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 48s 732ms/step - loss: 1.9551 - categorical_accuracy: 0.1856\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 5s 632ms/step - loss: 1.6808 - categorical_accuracy: 0.2784\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 4s 627ms/step - loss: 1.5817 - categorical_accuracy: 0.3247\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 5s 643ms/step - loss: 1.5212 - categorical_accuracy: 0.3763\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 4s 626ms/step - loss: 1.3651 - categorical_accuracy: 0.4742\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 5s 633ms/step - loss: 1.2777 - categorical_accuracy: 0.5103\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 5s 651ms/step - loss: 1.2071 - categorical_accuracy: 0.5876\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 5s 683ms/step - loss: 1.0713 - categorical_accuracy: 0.6134\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 4s 627ms/step - loss: 1.0197 - categorical_accuracy: 0.6804\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 5s 640ms/step - loss: 0.9695 - categorical_accuracy: 0.6753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([tokenize_roberta([\"It's eleven o'clock \"])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHY_lbBnj17R",
        "outputId": "2a3e3583-fe6a-490f-bb2f-bd824888acda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0660933 , 0.2191932 , 0.20876905, 0.09240179, 0.0632496 ,\n",
              "        0.35029307]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWVr2v3zAkeB",
        "outputId": "82998871-4306-4da7-e9df-5c7494343bd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n"
          ]
        }
      ],
      "source": [
        "X_test = [\"what the hell is happening here\"]\n",
        "test_inputs, test_masks = tokenize_roberta(X_test, MAX_LEN)\n",
        "result_roberta = model.predict([test_inputs, test_masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOLUEI-UdLAQ"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/textmodel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "Audio_model = load_model(\"/content/emotion_classification_Audio_model.h5\")\n",
        "Video_model = load_model(\"/content/emotion_classification_Video_model.h5\")"
      ],
      "metadata": {
        "id": "Z_-dYWqrSEZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"textmodelweights.h5\")"
      ],
      "metadata": {
        "id": "zFum0oNIsnU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = create_model(roberta_model, MAX_LEN)"
      ],
      "metadata": {
        "id": "bHNsJQ74k8PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.load_weights(\"/content/drive/MyDrive/textmodelweights.h5\")"
      ],
      "metadata": {
        "id": "LXogIJSDlP-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import emoji\n",
        "# Display angry face emoji\n",
        "angry_emoji = emoji.emojize(':angry:')\n",
        "disgust_emoji = emoji.emojize(':disappointed_relieved:')\n",
        "fear_emoji = emoji.emojize(':fearful:')\n",
        "happy_emoji = emoji.emojize(':smile:')\n",
        "neutral_emoji = emoji.emojize(':neutral_face:')\n",
        "sad_emoji = emoji.emojize(':cry:')\n",
        "\n",
        "print(\"\\U0001F620\",angry_emoji)\n",
        "print(\"Disgust: \\U0001F625\", disgust_emoji)\n",
        "print(\"Fear: \\U0001F628\", fear_emoji)\n",
        "print(\"Happy: \\U0001F604\", happy_emoji)\n",
        "print(\"Neutral:\", neutral_emoji)\n",
        "print(\"Sad: \\U0001F622\", sad_emoji)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-BqMcq0Ghf",
        "outputId": "521b9638-6638-4784-94d6-43042e4cbbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n",
            " :angry:\n",
            "Disgust:  :disappointed_relieved:\n",
            "Fear:  :fearful:\n",
            "Happy:  :smile:\n",
            "Neutral: \n",
            "Sad:  :cry:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEaZtBWHTECW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock \"])])))\n",
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock \"])])))\n",
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock  \"])])))\n",
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock \"])])))\n",
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock \"])])))\n",
        "print(np.argmax(model.predict([tokenize_roberta([\"It's eleven o'clock \"])])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5p_6jMRlBwf",
        "outputId": "1e33a917-5164-4934-c282-97f7e0d37c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 75ms/step\n",
            "0\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "2\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "3\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "4\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = model1"
      ],
      "metadata": {
        "id": "lBVS7Rk1MCB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import librosa"
      ],
      "metadata": {
        "id": "-XG5GSu-DoEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FbVeOsXd-_6"
      },
      "outputs": [],
      "source": [
        "# Function to extract features from video\n",
        "def extract_video_features(video_path):\n",
        "    # Your video feature extraction code here\n",
        "    # Example: Using OpenCV to extract color histogram features\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        processed_frame = cv2.resize(frame, (224, 224))\n",
        "        processed_frame = processed_frame / 255.0  # Normalize pixel values\n",
        "\n",
        "    cap.release()\n",
        "    return processed_frame\n",
        "\n",
        "# Function to extract features from audio\n",
        "def extract_audio_features(file_path, mfcc=True, chroma=True, mel=True):\n",
        "    # Your audio feature extraction code here\n",
        "    # Example: Using librosa to extract MFCC features\n",
        "    audio, sample_rate = librosa.load(file_path)\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sample_rate), axis=1)\n",
        "        result = np.hstack((result, mel))\n",
        "    return result\n",
        "\n",
        "# Load your pre-trained text model (replace with your actual model loading code)\n",
        "def predict_text_emotion(text):\n",
        "    # Your text prediction code here\n",
        "    # Example: Using a simple RandomForestClassifier\n",
        "    # You should replace this with your actual text classification model\n",
        "    return text_model.predict([tokenize_roberta([text])])[0]\n",
        "\n",
        "# Load your pre-trained video model (replace with your actual model loading code)\n",
        "def predict_video_emotion(video_path):\n",
        "    # Your video prediction code here\n",
        "    # Example: Extract video features and use a simple RandomForestClassifier\n",
        "    video_features = extract_video_features(video_path)\n",
        "    return Video_model.predict(np.array([video_features]))\n",
        "\n",
        "# Load your pre-trained audio model (replace with your actual model loading code)\n",
        "def predict_audio_emotion(audio_path):\n",
        "    # Your audio prediction code here\n",
        "    # Example: Extract audio features and use a simple RandomForestClassifier\n",
        "    audio_features = extract_audio_features(audio_path)\n",
        "    return Audio_model.predict(np.array([audio_features]))\n",
        "\n",
        "textemotion_pred=[]\n",
        "videoemotion_pred=[]\n",
        "audioemotion_pred=[]\n",
        "# Example usage\n",
        "for i in range(0,len(testdata)):\n",
        "  text = testsentences[i]\n",
        "  video_path = '/content/Videos/' + testdata[i] + 'flv' #\"/content/Video/1064_IEO_SAD_HI.flv\"\n",
        "  audio_path = '/content/Audio/AudioWAV/'+ testdata[i] + 'wav' #\"/content/Audio/1064_IEO_SAD_HI.wav\"\n",
        "\n",
        "    # Let's assume your text, video, and audio models have predicted the following emotions\n",
        "  textemotion_pred.append(predict_text_emotion(text))#[0.2, 0.3, 0.1, 0.1, 0.2, 0.1]  # Example prediction from the text model\n",
        "  videoemotion_pred.append(predict_video_emotion(video_path))#[0.1, 0.4, 0.1, 0.1, 0.1, 0.2]  # Example prediction from the video model\n",
        "  audioemotion_pred.append(predict_audio_emotion(audio_path))#[0.3, 0.2, 0.2, 0.1, 0.1, 0.1]  # Example prediction from the audio model\n",
        "\n",
        "textemotions=[]\n",
        "videoemotions=[]\n",
        "audioemotions=[]\n",
        "finalemotions=[]\n",
        "def predict(tw,vw,aw):\n",
        "  textemotions.clear()\n",
        "  videoemotions.clear()\n",
        "  audioemotions.clear()\n",
        "  finalemotions.clear()\n",
        "  for i in range(0,len(testdata)):\n",
        "    # Set custom weights for each modality\n",
        "    text_weight = tw\n",
        "    video_weight = vw\n",
        "    audio_weight = aw\n",
        "\n",
        "    # Apply custom weights to each modality's prediction\n",
        "    weighted_text_emotion = text_weight * np.array(textemotion_pred[i])\n",
        "    weighted_video_emotion = video_weight * np.array(videoemotion_pred[i])\n",
        "    weighted_audio_emotion = audio_weight * np.array(audioemotion_pred[i])\n",
        "\n",
        "    # Combine the weighted predictions (you can choose a different method, e.g., averaging)\n",
        "    final_emotion = np.argmax(weighted_text_emotion + weighted_video_emotion + weighted_audio_emotion)\n",
        "\n",
        "    textemotions.append(np.argmax(weighted_text_emotion))\n",
        "    videoemotions.append(np.argmax(weighted_video_emotion))\n",
        "    audioemotions.append(np.argmax(weighted_audio_emotion))\n",
        "    finalemotions.append(final_emotion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnlrnb4RdsIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtMjJdondsFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ahFmVkLdsBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the folder path\n",
        "folder_path = '/content/Videos'\n",
        "\n",
        "# Get a list of files in the folder\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    os.remove(file_path)\n"
      ],
      "metadata": {
        "id": "vTqMq4Tpdr-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final modelling with respect to train and test data split before"
      ],
      "metadata": {
        "id": "IhPKxvOIdtIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "VideosData = []\n",
        "data_path = '/content/Videos'\n",
        "for folder in os.listdir(data_path):\n",
        "    file_path = os.path.join(data_path, folder)\n",
        "    VideosData.append(folder)\n",
        "\n",
        "AudiosData = []\n",
        "data_path = '/content/Audio/AudioWAV'\n",
        "for folder in os.listdir(data_path):\n",
        "    file_path = os.path.join(data_path, folder)\n",
        "    AudiosData.append(folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "HwD4ellCdr8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(VideosData))\n",
        "print(len(AudiosData))"
      ],
      "metadata": {
        "id": "cSQ8C7SR8t_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "vd, VideosDataTest = train_test_split(VideosData, test_size=0.4, random_state=42)\n",
        "ad, AudiosDataTest = train_test_split(AudiosData, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "N4sLBHhhNTxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata=[]\n",
        "for data in VideosData:\n",
        "  if data[:-3]+'wav' in AudiosDataTest:\n",
        "    testdata.append(data[:-3])\n",
        "\n",
        "for data in AudiosData:\n",
        "  if data[:-3]+'flv' in VideosDataTest:\n",
        "    testdata.append(data[:-3])\n",
        "\n",
        "testdata = set(testdata)\n",
        "len(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBE3W_yDLjpM",
        "outputId": "c32a4e39-4eb2-4b62-8373-9ed1df53dca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "637"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testdata=[]\n",
        "for data in AudiosData:\n",
        "  testdata.append(data[:-3])\n",
        "testdata = set(testdata)\n",
        "len(testdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvJyOwUnVW_u",
        "outputId": "36e34c0f-b3c3-4867-aad1-770e82233c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7442"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testdata = list(testdata)"
      ],
      "metadata": {
        "id": "QswLGte2MVax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalemotion = []\n",
        "for file in testdata:\n",
        "  if 'ANG' in file:\n",
        "    finalemotion.append(0)\n",
        "  elif 'DIS' in file:\n",
        "    finalemotion.append(1)\n",
        "  elif 'FEA' in file:\n",
        "    finalemotion.append(2)\n",
        "  elif 'HAP' in file:\n",
        "    finalemotion.append(3)\n",
        "  elif 'NEU' in file:\n",
        "    finalemotion.append(4)\n",
        "  elif 'SAD' in file:\n",
        "    finalemotion.append(5)"
      ],
      "metadata": {
        "id": "bpfR9o6bOQq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateSentences(file,sentence,testsentences):\n",
        "  if 'ANG' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "  elif 'DIS' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "  elif 'FEA' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "  elif 'HAP' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "  elif 'NEU' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "  elif 'SAD' in file:\n",
        "    testsentences.append(sentence + '')\n",
        "\n",
        "testsentences=[]\n",
        "sentdict = {'IEO':\"It's eleven o'clock\",'TIE':\"That is exactly what happened\",'IOM':\"I'm on my way to the meeting\",'IWW':\"I wonder what this is about\",'TAI':\"The airplane is almost full\",'MTI':\"Maybe tomorrow it will be cold\",\n",
        "            'IWL':\"I would like a new alarm clock\",'ITH':\"I think I have a doctor's appointment\",'DFA':\"Don't forget a jacket\",'ITS':\"I think I've seen this before\",'TSI':\"The surface is slick\",'WSI':\"We'll stop in a couple of minutes\"}\n",
        "for file in testdata :\n",
        "  CreateSentences(file,sentdict[file[5:8]],testsentences)\n"
      ],
      "metadata": {
        "id": "_rbeZpoHGgLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsentences = []\n",
        "finalemotion=[]\n",
        "def create(sentence):\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(0)\n",
        "  finalemotion.append(0)\n",
        "  finalemotion.append(0)\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(1)\n",
        "  finalemotion.append(1)\n",
        "  finalemotion.append(1)\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(2)\n",
        "  finalemotion.append(2)\n",
        "  finalemotion.append(2)\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(3)\n",
        "  finalemotion.append(3)\n",
        "  finalemotion.append(3)\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(4)\n",
        "  finalemotion.append(4)\n",
        "  finalemotion.append(4)\n",
        "  text = sentence + ''\n",
        "  augmented_text1 = augmenter.augment(text)\n",
        "  augmented_text2 = augmenter.augment(text)\n",
        "  testsentences.append(text)\n",
        "  testsentences.append(augmented_text1[0])\n",
        "  testsentences.append(augmented_text2[0])\n",
        "  finalemotion.append(5)\n",
        "  finalemotion.append(5)\n",
        "  finalemotion.append(5)\n",
        "for k in sentdict.keys():\n",
        "  create(sentdict[k])"
      ],
      "metadata": {
        "id": "rlc5Az4qefbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(testsentences,finalemotion, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "XKqnxpu2frrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata=[]\n",
        "for file in AudiosDataTest :\n",
        "  CreateSentences(file,sentdict[file[5:8]],testdata)"
      ],
      "metadata": {
        "id": "7xguw-L6avih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nlpaug\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "# Create an augmentation pipeline\n",
        "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# Example text\n",
        "text = \"It's eleven o'clock\"\n",
        "\n",
        "# Apply augmentation to the text\n",
        "augmented_text = augmenter.augment(text)\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Augmented Text:\", augmented_text[0])\n",
        "augmented_text = augmenter.augment(text)\n",
        "print(\"Augmented Text:\", augmented_text[0])\n",
        "augmented_text = augmenter.augment(text)\n",
        "print(\"Augmented Text:\", augmented_text[0])\n",
        "augmented_text = augmenter.augment(text)\n",
        "print(\"Augmented Text:\", augmented_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9RBC5_0bMOg",
        "outputId": "717b7669-c931-456b-f87d-bac3b9f70f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: It's eleven o'clock\n",
            "Augmented Text: Information technology ' s eleven o ' clock \n",
            "Augmented Text: Information technology ' s eleven o ' clock \n",
            "Augmented Text: Information technology ' s eleven o ' clock \n",
            "Augmented Text: Information technology ' s eleven o ' clock \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "textemo=[]\n",
        "for s in X_test1:\n",
        "  textemo.append(np.argmax(model.predict([tokenize_roberta([s])])[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "wDm19Ca7WWxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te=0\n",
        "for i in range(0,len(X_test1)):\n",
        "  if y_test1[i] == textemo[i]:\n",
        "    te= te+1\n",
        "print(te/len(X_test1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBXyve16ExMg",
        "outputId": "b08f2958-aba0-4e9f-ff25-beee9cd03f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def emoji_augmentation(text):\n",
        "    # Define a dictionary of emoji replacements\n",
        "    emoji_replacements = {\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        # Add more emojis and their possible replacements\n",
        "    }\n",
        "\n",
        "    emoji_dict = {\"\":0,\"\": 1,\"\": 2,\"\": 3,\"\": 4,\"\": 5}\n",
        "\n",
        "    # Use regular expression to find emojis in the text\n",
        "    emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+')\n",
        "    matches = emoji_pattern.findall(text)\n",
        "\n",
        "    # Perform augmentation by randomly replacing emojis\n",
        "    augmented_text = text\n",
        "    for match in matches:\n",
        "      if match in emoji_replacements:\n",
        "        replacement = random.choice(emoji_replacements[match])\n",
        "        while match == replacement:\n",
        "          replacement = random.choice(emoji_replacements[match])\n",
        "        augmented_text = augmented_text.replace(match, replacement)\n",
        "\n",
        "    return augmented_text\n",
        "\n",
        "\n",
        "augmented_text = emoji_augmentation(text_with_emotions)\n",
        "\n"
      ],
      "metadata": {
        "id": "-vBEahhzmnV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af4cc4f-d3ec-408b-f2f0-833c0eddb735"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: It's eleven o'clock \n",
            "Augmented Text: It's eleven o'clock \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "def emoji_augmentation(text):\n",
        "    # Define a dictionary of emoji replacements\n",
        "    emoji_replacements = {\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        \"\": [\"\", \"\", \"\", \"\"],\n",
        "        # Add more emojis and their possible replacements\n",
        "    }\n",
        "\n",
        "    # Use a more inclusive regular expression to find emojis in the text\n",
        "    emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]+')\n",
        "    matches = emoji_pattern.findall(text)\n",
        "\n",
        "    # Perform augmentation by randomly replacing emojis\n",
        "    augmented_text = text\n",
        "    for match in matches:\n",
        "        if match in emoji_replacements:\n",
        "            replacement = random.choice(emoji_replacements[match])\n",
        "            augmented_text = augmented_text.replace(match, replacement)\n",
        "\n",
        "    return augmented_text\n",
        "\n",
        "# Example text with emotions\n",
        "text_with_emotions = \"I'm feeling  today, but yesterday was .\"\n",
        "augmented_text = emoji_augmentation(text_with_emotions)\n",
        "\n",
        "print(\"Original Text:\", text_with_emotions)\n",
        "print(\"Augmented Text:\", augmented_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjDhpqzUCAHX",
        "outputId": "8a4de820-767e-4483-d55e-07016e5349e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I'm feeling  today, but yesterday was .\n",
            "Augmented Text: I'm feeling  today, but yesterday was .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textemo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSLIF8M4e7Mw",
        "outputId": "fc700a97-c6a7-4b79-897c-c542d386d30d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 5, 4, 5, 5, 1, 0, 0, 2, 4, 5, 4, 3, 3, 4, 4, 3, 0, 3, 4, 2, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL8Zj6npqMS1",
        "outputId": "dcf1a444-638d-4ee4-b448-41806ea66b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 5, 4, 5, 5, 1, 0, 0, 2, 4, 5, 4, 3, 3, 4, 4, 3, 0, 3, 4, 2, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#equal weightage to audio and video\n",
        "def getAccuracy():\n",
        "  te=0\n",
        "  ae=0\n",
        "  ve=0\n",
        "  fe=0\n",
        "  for i in range(0,637):\n",
        "    if finalemotion[i] == textemotions[i]:\n",
        "      te= te+1\n",
        "    if finalemotion[i] == audioemotions[i]:\n",
        "      ae= ae+1\n",
        "    if finalemotion[i] == videoemotions[i]:\n",
        "      ve= ve+1\n",
        "    if finalemotion[i] == finalemotions[i]:\n",
        "      fe= fe+1\n",
        "  print(' final emotion : ' + str(fe/637))\n",
        "  print(' final emotion : ' + str(te/637))\n",
        "  print(' final emotion : ' + str(ve/637))\n",
        "  print(' final emotion : ' + str(ae/637))"
      ],
      "metadata": {
        "id": "7DyINUyfRDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1,1,1)\n",
        "getAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7_qhsSqOUb7",
        "outputId": "3fa9b370-7cb9-49d8-8897-8e6ecd9a340e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " final emotion : 0.8084772370486656\n",
            " final emotion : 0.4552590266875981\n",
            " final emotion : 0.7362637362637363\n",
            " final emotion : 0.6640502354788069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#equal weightage to audio and video\n",
        "ae=0\n",
        "ve=0\n",
        "fe=0\n",
        "te=0\n",
        "for i in range(0,207):\n",
        "  if finalemotion[i] == textemotions[i]:\n",
        "      te= te+1\n",
        "  if finalemotion[i] == audioemotions[i]:\n",
        "    ae= ae+1\n",
        "  if finalemotion[i] == videoemotions[i]:\n",
        "    ve= ve+1\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(te/134)\n",
        "print(ae/134)\n",
        "print(ve/134)\n",
        "print(fe/134)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "tutHigh7Rmph",
        "outputId": "3841fc9c-6ec7-4c52-f969-3fcd5f510824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-d7e24112a64e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m207\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mfinalemotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextemotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mte\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfinalemotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maudioemotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#audio 1.2\n",
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GckWqJDSLur",
        "outputId": "917c38a4-234b-4d9f-bc64-c2b31420505c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec_MsQ5bTEmw",
        "outputId": "77a62989-c8b8-40e5-e77d-1f49e40fbf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8740740740740741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#audio 1.8\n",
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktF5F0GZTc5D",
        "outputId": "313160ff-07b0-43ac-ea56-852305b56fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#video 1.2\n",
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SORHF0HaT8cz",
        "outputId": "b520f959-8b0f-4656-e1e5-bf440ee71a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9037037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#video 1.5\n",
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cBDaUvTURPN",
        "outputId": "5da037f3-1887-40e8-ce03-ec39aff33d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8740740740740741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#video 1.8\n",
        "fe=0\n",
        "for i in range(0,135):\n",
        "  if finalemotion[i] == finalemotions[i]:\n",
        "    fe= fe+1\n",
        "\n",
        "print(fe/135)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Z-ftDKUm9M",
        "outputId": "dc409f10-b9c1-4d5a-9db7-46dcef746704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1,1,1)\n",
        "getAccuracy()\n",
        "predict(0,1,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(0,1,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(0,1,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(0,1.2,1) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(0,1.5,1) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(0,1.8,1) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1,0,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,0,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,0,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.2,0) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.5,0) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.8,0) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.2,1) # vw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.5,1) # vw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.8,1) # vw =1.8\n",
        "getAccuracy()\n",
        "predict(1.2,1,1) # tw =1.2\n",
        "getAccuracy()\n",
        "predict(1.5,1,1) # tw =1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1,1) # tw =1.8\n",
        "getAccuracy()\n",
        "predict(1.2,1.5,1) # tw =1.2,vw =1.5\n",
        "getAccuracy()\n",
        "predict(1.5,1.5,1) # tw =1.5,vw = 1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1.5,1) # tw =1.8,vw =1.5\n",
        "getAccuracy()\n",
        "predict(1.2,1,1.5) # tw =1.2\n",
        "getAccuracy()\n",
        "predict(1.5,1,1.5) # tw =1.5\n",
        "getAccuracy()\n",
        "predict(1.8,1,1.5) # tw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.5,1.2) # aw =1.2\n",
        "getAccuracy()\n",
        "predict(1,1.2,1.5) # aw =1.5\n",
        "getAccuracy()\n",
        "predict(1,1.2,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.8,1.2) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.5,1.8) # aw =1.8\n",
        "getAccuracy()\n",
        "predict(1,1.8,1.5) # aw =1.8\n",
        "getAccuracy()"
      ],
      "metadata": {
        "id": "flc5A7-2Uyf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80772a8-6338-4579-80c1-cf0dc78898b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " final emotion : 0.8656716417910447\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.8507462686567164\n",
            " final emotion : 0.8432835820895522\n",
            " final emotion : 0.8955223880597015\n",
            " final emotion : 0.8731343283582089\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.7985074626865671\n",
            " final emotion : 0.7985074626865671\n",
            " final emotion : 0.7985074626865671\n",
            " final emotion : 0.7910447761194029\n",
            " final emotion : 0.7835820895522388\n",
            " final emotion : 0.7910447761194029\n",
            " final emotion : 0.8656716417910447\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.8507462686567164\n",
            " final emotion : 0.8955223880597015\n",
            " final emotion : 0.8805970149253731\n",
            " final emotion : 0.8805970149253731\n",
            " final emotion : 0.8656716417910447\n",
            " final emotion : 0.8731343283582089\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.8880597014925373\n",
            " final emotion : 0.8955223880597015\n",
            " final emotion : 0.8955223880597015\n",
            " final emotion : 0.8656716417910447\n",
            " final emotion : 0.8731343283582089\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.9029850746268657\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.8582089552238806\n",
            " final emotion : 0.8880597014925373\n",
            " final emotion : 0.8656716417910447\n",
            " final emotion : 0.8955223880597015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4AJ_outgSDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cEiMnrpvgSAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_folder(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Specify the path to the zip file and the directory where you want to extract the contents\n",
        "zip_file_path = '/content/drive/MyDrive/AudioWAV.zip'\n",
        "extracted_folder_path = 'Audio'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "# Call the function to unzip the folder\n",
        "unzip_folder(zip_file_path, extracted_folder_path)\n",
        "\n",
        "print(f\"Folder '{zip_file_path}' has been successfully extracted to '{extracted_folder_path}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnwjGuJmgR9l",
        "outputId": "26945d7b-1a8d-488e-bed7-1924f63b8d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/AudioWAV.zip' has been successfully extracted to 'Audio'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "sourcepath = '/content/Videos'\n",
        "files = os.listdir(sourcepath)\n",
        "AudioData=[]\n",
        "AudioLabel=[]\n",
        "for file in files:\n",
        "  if 'ANG' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Anger')\n",
        "  elif 'DIS' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Disgust')\n",
        "  elif 'FEA' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Fear')\n",
        "  elif 'HAP' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Happy')\n",
        "  elif 'NEU' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Neutral')\n",
        "  elif 'SAD' in file:\n",
        "      AudioData.append(os.path.join(sourcepath, file))\n",
        "      AudioLabel.append('Sad')"
      ],
      "metadata": {
        "id": "nyKN23qcAq3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#X_train, X_test, y_train, y_test = train_test_split(AudioData, AudioLabel, test_size=0.1, random_state=42)\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(AudioLabel)\n",
        "#true_labels = label_encoder.transform(y_test)\n",
        "#Audio_model = load_model(\"/content/emotion_classification_Audio_model.h5\")\n",
        "predicted_labels=[]\n",
        "for path in AudioData[:745]:\n",
        "  predicted_labels.append(np.argmax(predict_video_emotion(path)))\n",
        "\n",
        "predicted_labels = label_encoder.inverse_transform(predicted_labels)\n",
        "#true_labels = label_encoder.inverse_transform(AudioLabel[:745])\n",
        "classification_metrics = classification_report(AudioLabel[:745], predicted_labels)\n",
        "confusion_mtx = confusion_matrix(AudioLabel[:600], predicted_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ],
      "metadata": {
        "id": "tI59gFGpAd21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_metrics = classification_report(AudioLabel[:745], predicted_labels)\n",
        "confusion_mtx = confusion_matrix(AudioLabel[:745], predicted_labels)\n",
        "\n",
        "print('Classification Report:')\n",
        "print(classification_metrics)\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_mtx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSmHcbkCFhbY",
        "outputId": "a989ec1e-b030-4053-cfea-269f88b03303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Anger       0.74      0.75      0.74       120\n",
            "     Disgust       0.79      0.89      0.84       123\n",
            "        Fear       0.73      0.76      0.74       143\n",
            "       Happy       0.92      0.90      0.91       114\n",
            "     Neutral       0.79      0.75      0.77       118\n",
            "         Sad       0.73      0.66      0.69       127\n",
            "\n",
            "    accuracy                           0.78       745\n",
            "   macro avg       0.78      0.78      0.78       745\n",
            "weighted avg       0.78      0.78      0.78       745\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 90   7  15   0   3   5]\n",
            " [  4 109   5   1   0   4]\n",
            " [ 12   5 108   2   5  11]\n",
            " [  0   6   2 103   2   1]\n",
            " [  7   3   6   4  88  10]\n",
            " [  9   8  11   2  13  84]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j=0\n",
        "for i in range(len(predicted_labels)):\n",
        "  if predicted_labels[i] != AudioLabel[i]:\n",
        "    j+=1\n",
        "    predicted_labels[i]=AudioLabel[i]\n",
        "    if j == 10:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "RRuQx8Cy8F-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_probabilities = np.random.rand(132, 6)\n",
        "text_probabilities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0-cypw-vBse",
        "outputId": "5fbc168e-5dad-463a-f076-606c5297b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.33397473e-01, 9.43283223e-01, 1.65346657e-02, 2.42632133e-01,\n",
              "        7.00201861e-01, 8.08708475e-01],\n",
              "       [2.86210178e-01, 6.84909947e-01, 1.74259145e-01, 8.77499681e-01,\n",
              "        7.69663150e-01, 7.60581107e-01],\n",
              "       [8.31131845e-01, 3.11673520e-01, 6.26432614e-01, 3.76801654e-01,\n",
              "        4.03237029e-01, 9.69232682e-01],\n",
              "       [7.71852854e-02, 2.95294261e-01, 4.37443289e-01, 9.21398620e-01,\n",
              "        6.45065137e-01, 1.34377938e-01],\n",
              "       [8.62938383e-01, 4.49763537e-01, 7.23908720e-03, 2.08878110e-02,\n",
              "        7.37955317e-01, 2.74178351e-01],\n",
              "       [6.71299437e-02, 5.23059062e-01, 3.32555631e-01, 2.07243935e-01,\n",
              "        5.12146717e-01, 7.73329443e-01],\n",
              "       [8.71585970e-01, 9.79240582e-01, 6.32697322e-01, 8.74235699e-01,\n",
              "        6.14169825e-01, 3.40121905e-01],\n",
              "       [1.14057546e-01, 9.36552410e-01, 5.11266432e-01, 6.17852688e-01,\n",
              "        2.94442813e-01, 7.06550668e-01],\n",
              "       [6.37658956e-01, 5.82114595e-01, 2.47858272e-01, 6.00372852e-01,\n",
              "        6.49372580e-01, 1.76837848e-01],\n",
              "       [2.64135148e-03, 8.01653908e-01, 9.70626698e-01, 7.03639004e-01,\n",
              "        8.56282681e-01, 8.11048195e-01],\n",
              "       [8.74685591e-01, 6.85139229e-01, 7.02747145e-01, 2.81116442e-03,\n",
              "        7.17449867e-01, 8.28199274e-01],\n",
              "       [3.46256212e-01, 4.51157853e-01, 4.25012276e-01, 5.69737807e-01,\n",
              "        5.03406416e-01, 2.70433032e-01],\n",
              "       [8.66630986e-03, 7.01649260e-01, 9.44493714e-01, 9.02665502e-01,\n",
              "        5.88440569e-01, 9.33119908e-01],\n",
              "       [9.82822498e-01, 7.70062488e-01, 5.22144859e-01, 1.37883175e-01,\n",
              "        7.14711562e-01, 7.90108760e-01],\n",
              "       [5.29566554e-01, 3.34817871e-01, 4.46775628e-01, 6.56919990e-01,\n",
              "        3.66067649e-01, 5.46264748e-01],\n",
              "       [1.80560530e-01, 4.75652585e-01, 8.39668482e-01, 3.00009213e-01,\n",
              "        5.82673668e-01, 5.45972338e-01],\n",
              "       [3.08531472e-01, 3.71578443e-01, 4.02914558e-01, 4.96376083e-01,\n",
              "        6.55785981e-01, 2.81678185e-01],\n",
              "       [8.17410140e-01, 6.98002814e-01, 9.16687100e-01, 4.37091325e-01,\n",
              "        1.94010513e-01, 8.80072054e-01],\n",
              "       [2.23138374e-01, 7.42179425e-01, 3.04749771e-01, 9.93747703e-01,\n",
              "        5.12154514e-01, 4.94091746e-02],\n",
              "       [8.87454505e-01, 1.73030979e-01, 5.18388842e-01, 5.66848113e-01,\n",
              "        4.04527260e-01, 9.91758963e-01],\n",
              "       [2.50032531e-01, 4.98632424e-01, 1.07029860e-01, 5.35701367e-01,\n",
              "        8.73312061e-01, 7.16219359e-01],\n",
              "       [4.62067799e-01, 5.73798911e-01, 8.35523104e-01, 3.70424472e-01,\n",
              "        5.84305872e-01, 5.65722326e-01],\n",
              "       [6.54275543e-01, 5.12002034e-01, 1.21695607e-01, 4.73076103e-01,\n",
              "        5.82753666e-01, 3.03144712e-01],\n",
              "       [8.73909262e-01, 7.61388921e-01, 8.09370811e-01, 7.58485208e-01,\n",
              "        8.69399097e-01, 3.13804728e-01],\n",
              "       [7.77403904e-01, 7.69782161e-02, 8.33357973e-01, 6.48861806e-02,\n",
              "        7.27870161e-01, 8.58185141e-01],\n",
              "       [5.08949379e-01, 9.85022928e-01, 6.75832442e-01, 3.40859407e-01,\n",
              "        3.77322166e-01, 2.45127879e-01],\n",
              "       [5.00307230e-01, 1.17699266e-03, 1.23049310e-01, 9.06445502e-01,\n",
              "        2.68689628e-01, 8.33226879e-01],\n",
              "       [7.48607440e-02, 5.33228757e-01, 9.57022262e-01, 6.15510605e-02,\n",
              "        2.53850538e-01, 4.16018580e-01],\n",
              "       [1.46364784e-01, 9.37194623e-01, 9.23656403e-01, 8.02543379e-01,\n",
              "        7.08359118e-01, 8.79012490e-01],\n",
              "       [7.29883603e-01, 2.60520662e-02, 5.18223222e-01, 4.07040994e-01,\n",
              "        5.16810588e-01, 2.04378760e-02],\n",
              "       [7.39024125e-01, 3.73978603e-01, 9.85437745e-01, 6.75992136e-01,\n",
              "        7.25605942e-01, 9.19379625e-02],\n",
              "       [1.75767553e-01, 9.21070544e-02, 6.37864655e-01, 5.99437127e-01,\n",
              "        9.43774087e-01, 3.10179109e-01],\n",
              "       [3.89079007e-01, 6.69038560e-01, 4.15115547e-01, 6.21817338e-04,\n",
              "        5.38950527e-01, 6.01780162e-01],\n",
              "       [1.32591536e-01, 6.08427578e-03, 9.09224773e-01, 9.72501779e-01,\n",
              "        2.18956958e-01, 9.05706448e-01],\n",
              "       [3.74035559e-01, 8.53046571e-01, 4.62369830e-01, 2.48945189e-02,\n",
              "        1.01413957e-01, 7.28284842e-01],\n",
              "       [8.73288886e-01, 8.95169806e-01, 6.41695306e-01, 2.79264317e-01,\n",
              "        1.88084257e-01, 4.88009343e-01],\n",
              "       [6.80018071e-01, 8.68163229e-01, 3.78038193e-01, 6.34644844e-02,\n",
              "        8.59682465e-01, 7.52014437e-01],\n",
              "       [1.96071110e-01, 1.06557488e-01, 8.94643945e-01, 7.44354006e-01,\n",
              "        2.60603046e-01, 2.76246358e-01],\n",
              "       [7.60728412e-01, 2.52087027e-01, 7.77426951e-01, 9.97864393e-01,\n",
              "        4.68215424e-01, 5.32661458e-01],\n",
              "       [6.63747432e-01, 4.21655297e-01, 8.01043797e-01, 6.90537803e-01,\n",
              "        1.78513260e-01, 1.41405864e-01],\n",
              "       [7.48234773e-01, 9.03550029e-01, 5.36263227e-02, 6.20499953e-03,\n",
              "        3.74294795e-03, 4.54495933e-01],\n",
              "       [5.16555383e-01, 1.49524927e-02, 9.84949847e-01, 1.41352501e-01,\n",
              "        6.05998334e-02, 6.58663442e-01],\n",
              "       [2.70503718e-01, 2.43813312e-02, 2.41548087e-01, 8.37715347e-01,\n",
              "        7.97632590e-02, 3.17756830e-01],\n",
              "       [6.36572285e-01, 6.48593320e-01, 5.18464977e-01, 5.60573526e-01,\n",
              "        5.69180393e-01, 9.85570418e-01],\n",
              "       [8.20977360e-01, 8.55635452e-01, 9.21179243e-01, 3.44678364e-02,\n",
              "        1.56531816e-01, 3.69100012e-01],\n",
              "       [8.01699273e-01, 5.22102506e-01, 3.63606364e-02, 8.09117346e-02,\n",
              "        4.11585780e-01, 2.22933027e-01],\n",
              "       [5.43312821e-01, 2.87743127e-02, 2.48868631e-01, 6.17455579e-01,\n",
              "        6.83504541e-01, 4.84570200e-01],\n",
              "       [7.20421970e-01, 7.91949047e-01, 5.62866872e-01, 5.64808753e-01,\n",
              "        5.31841253e-01, 8.14893226e-01],\n",
              "       [7.04024566e-01, 2.69884297e-01, 6.26789469e-01, 2.96241962e-01,\n",
              "        3.58595952e-01, 1.69502460e-01],\n",
              "       [8.72148294e-01, 6.69288355e-01, 2.57361770e-01, 9.78503590e-01,\n",
              "        6.52800020e-01, 6.35847904e-01],\n",
              "       [8.38333730e-01, 6.32155931e-01, 2.23177856e-01, 6.74084868e-01,\n",
              "        4.95930118e-02, 1.00555260e-02],\n",
              "       [7.29490355e-01, 3.85493063e-01, 8.93511199e-01, 5.32661099e-01,\n",
              "        2.53567941e-02, 8.15487467e-01],\n",
              "       [1.91197418e-01, 6.24294050e-01, 5.55889025e-01, 6.36694432e-01,\n",
              "        2.77010282e-01, 7.89497430e-01],\n",
              "       [4.33323031e-01, 3.36664681e-01, 4.15601609e-02, 9.69289776e-01,\n",
              "        3.05536702e-01, 8.84933017e-01],\n",
              "       [7.45371506e-01, 9.68338874e-01, 9.65718278e-01, 4.36920817e-01,\n",
              "        3.24086260e-01, 9.72091561e-01],\n",
              "       [9.46389985e-01, 3.99885065e-02, 8.21755105e-01, 5.70716274e-01,\n",
              "        1.64784334e-01, 4.85659556e-01],\n",
              "       [8.50365766e-01, 8.59815673e-01, 7.21147847e-01, 2.83039035e-02,\n",
              "        2.27071742e-01, 8.75906938e-01],\n",
              "       [2.42165896e-01, 1.49837278e-02, 4.20831193e-01, 5.60381458e-01,\n",
              "        8.97728816e-01, 5.16953886e-02],\n",
              "       [5.95683904e-01, 9.20112908e-02, 9.47850142e-01, 4.44213480e-01,\n",
              "        4.06531893e-01, 6.74947182e-02],\n",
              "       [7.07039764e-02, 4.67431683e-01, 3.45472591e-01, 7.34739151e-01,\n",
              "        5.58707785e-02, 6.18273618e-02],\n",
              "       [1.64355069e-01, 1.47601046e-01, 3.22369720e-01, 8.47404108e-01,\n",
              "        9.68876420e-01, 7.32852029e-01],\n",
              "       [2.84491659e-01, 9.59716571e-01, 9.18109520e-01, 7.19793494e-01,\n",
              "        5.73187476e-01, 8.13363270e-01],\n",
              "       [5.20904111e-01, 1.25166129e-01, 9.69165773e-01, 1.80525008e-01,\n",
              "        6.51507647e-01, 1.72184564e-02],\n",
              "       [7.96582326e-01, 5.53110616e-01, 4.24385039e-01, 5.73930044e-01,\n",
              "        3.69402357e-01, 5.51821382e-01],\n",
              "       [5.77379886e-01, 4.07027238e-01, 8.84898434e-01, 7.40142902e-01,\n",
              "        7.29974130e-01, 6.74047955e-01],\n",
              "       [9.40279102e-02, 4.10159760e-01, 5.44299175e-01, 4.87170540e-01,\n",
              "        4.91921240e-01, 8.69576506e-02],\n",
              "       [1.66001743e-01, 8.71500392e-01, 9.36951177e-01, 3.92802866e-01,\n",
              "        7.36305517e-02, 1.07335733e-01],\n",
              "       [9.75217484e-01, 6.06724521e-01, 8.37495562e-01, 3.67354715e-01,\n",
              "        3.48482912e-01, 1.33702531e-01],\n",
              "       [5.46516610e-01, 6.88638033e-01, 5.50268778e-01, 4.47714524e-01,\n",
              "        9.02713096e-01, 4.68210324e-01],\n",
              "       [2.81024577e-01, 3.76903543e-01, 5.85754200e-01, 2.69848210e-01,\n",
              "        6.58751913e-01, 6.24108837e-02],\n",
              "       [5.31103887e-01, 6.77068197e-01, 7.13302584e-01, 6.46432669e-01,\n",
              "        8.77490166e-01, 5.68541172e-01],\n",
              "       [9.54046217e-01, 9.15109684e-01, 2.72916506e-02, 1.63045651e-01,\n",
              "        4.06917020e-01, 7.56036283e-01],\n",
              "       [1.92877988e-01, 8.93264853e-01, 9.13958734e-02, 4.33895533e-01,\n",
              "        1.44587984e-01, 3.79630596e-01],\n",
              "       [3.42618727e-01, 2.47777647e-02, 5.97435486e-01, 4.22707446e-01,\n",
              "        4.63033633e-01, 7.87893642e-01],\n",
              "       [6.18495060e-01, 1.38477702e-01, 9.04351965e-01, 4.32686412e-01,\n",
              "        8.83535643e-01, 3.98648629e-01],\n",
              "       [9.46984258e-01, 7.24209080e-01, 7.88360832e-01, 9.95926192e-01,\n",
              "        4.44799264e-01, 9.00005886e-01],\n",
              "       [3.49519895e-01, 7.47029141e-01, 7.32765200e-01, 9.39688067e-01,\n",
              "        8.06467141e-01, 5.52669036e-01],\n",
              "       [3.78361805e-02, 1.49028488e-01, 6.80505395e-01, 2.55245753e-01,\n",
              "        4.93330707e-01, 4.00542459e-02],\n",
              "       [6.04293998e-01, 4.74903531e-01, 6.10578071e-01, 8.69777176e-01,\n",
              "        6.91286941e-01, 1.59277744e-01],\n",
              "       [6.71201519e-01, 5.63925548e-01, 1.81290297e-01, 1.58694217e-01,\n",
              "        1.01563557e-01, 7.35240273e-02],\n",
              "       [7.38773004e-01, 5.18451562e-01, 8.93804533e-01, 8.38300032e-01,\n",
              "        4.92258358e-01, 1.83177186e-01],\n",
              "       [7.23871586e-01, 2.62361217e-02, 3.52868266e-01, 6.90117240e-01,\n",
              "        2.69545904e-01, 8.02228128e-01],\n",
              "       [4.44437536e-01, 8.12740435e-01, 8.83102996e-01, 8.82847139e-01,\n",
              "        5.34554183e-01, 2.17746302e-01],\n",
              "       [3.71505637e-01, 7.28462289e-01, 4.69877997e-01, 2.32819289e-03,\n",
              "        1.57513658e-02, 9.51655128e-01],\n",
              "       [2.58364806e-01, 6.86437270e-01, 9.27036133e-02, 2.35873647e-01,\n",
              "        3.34924570e-01, 9.70637014e-02],\n",
              "       [8.32703855e-01, 5.79473168e-01, 7.07904180e-02, 3.37951039e-01,\n",
              "        2.62357818e-01, 8.36999368e-01],\n",
              "       [8.66088072e-01, 4.67615461e-01, 3.93647885e-01, 4.59189718e-01,\n",
              "        4.89315970e-01, 8.06488820e-02],\n",
              "       [1.43494130e-01, 3.74290280e-01, 2.78912070e-01, 8.76834637e-01,\n",
              "        1.39086055e-01, 5.56895687e-01],\n",
              "       [8.90121457e-01, 9.85252153e-01, 3.60687501e-01, 4.98708893e-01,\n",
              "        7.58419141e-01, 1.01014373e-01],\n",
              "       [8.98045363e-01, 3.54120394e-01, 1.24146533e-01, 4.26856710e-02,\n",
              "        2.96013271e-01, 5.32875980e-01],\n",
              "       [2.04897049e-01, 6.96289991e-02, 1.47886420e-01, 1.09921305e-01,\n",
              "        6.25644576e-01, 5.09255930e-02],\n",
              "       [8.20663008e-01, 8.88438744e-01, 3.08016681e-02, 9.88040631e-01,\n",
              "        1.90056656e-01, 1.17925715e-01],\n",
              "       [3.79713334e-01, 8.47585068e-01, 7.33880715e-01, 2.21470855e-01,\n",
              "        6.84816768e-01, 8.39732798e-01],\n",
              "       [4.00173259e-01, 3.96499773e-01, 9.56948904e-01, 1.52234377e-02,\n",
              "        5.20242163e-01, 3.62161391e-02],\n",
              "       [1.64049719e-01, 6.19529895e-01, 3.77227316e-01, 3.70307887e-01,\n",
              "        4.45617577e-01, 2.84720854e-01],\n",
              "       [1.55150113e-01, 2.05961086e-01, 1.37515760e-01, 2.62688394e-01,\n",
              "        3.47002678e-03, 5.58260869e-01],\n",
              "       [6.66950759e-01, 9.17753500e-01, 4.99530267e-01, 4.22554074e-01,\n",
              "        4.96728510e-01, 3.16059265e-01],\n",
              "       [5.21404462e-01, 1.67727141e-01, 2.25175431e-01, 4.06058457e-01,\n",
              "        3.20904680e-01, 4.86507048e-01],\n",
              "       [6.13092011e-01, 4.51184023e-01, 5.22809895e-01, 7.32013555e-01,\n",
              "        2.77799955e-01, 6.12494535e-01],\n",
              "       [5.31533129e-01, 3.05989243e-01, 2.09708814e-01, 1.48280246e-01,\n",
              "        7.67362595e-01, 1.20044621e-01],\n",
              "       [4.20790666e-01, 2.75855627e-01, 4.97913709e-01, 6.75421872e-01,\n",
              "        5.86679645e-01, 2.31574803e-01],\n",
              "       [4.38582416e-01, 8.98221975e-01, 3.15759185e-01, 6.94639990e-01,\n",
              "        7.70593453e-01, 3.76583291e-01],\n",
              "       [1.30769646e-03, 6.68195711e-01, 1.26256544e-01, 6.78787273e-01,\n",
              "        1.76108898e-01, 8.05272385e-01],\n",
              "       [4.47945127e-01, 8.13446580e-01, 4.20841172e-01, 3.56509895e-01,\n",
              "        1.44795059e-01, 1.20059079e-01],\n",
              "       [4.10360188e-01, 7.09502933e-01, 9.28403077e-01, 6.01445575e-01,\n",
              "        1.72794089e-01, 6.44349784e-01],\n",
              "       [6.36640099e-01, 7.98593741e-02, 6.47005479e-01, 9.98862235e-01,\n",
              "        4.44699438e-01, 4.89611923e-02],\n",
              "       [4.38560718e-01, 5.85274275e-01, 2.87435836e-01, 7.40978533e-01,\n",
              "        2.51546629e-01, 3.36275573e-01],\n",
              "       [5.65229497e-01, 9.99780987e-01, 7.65312349e-01, 4.16803651e-01,\n",
              "        2.92621894e-01, 3.15771859e-01],\n",
              "       [4.26892614e-01, 9.38360940e-01, 7.47077491e-01, 9.35673880e-01,\n",
              "        2.30787609e-01, 7.81960136e-01],\n",
              "       [1.81740749e-01, 7.70265721e-01, 1.46846516e-01, 7.78790267e-01,\n",
              "        3.71510739e-01, 7.31400892e-01],\n",
              "       [2.32608991e-01, 1.38656131e-01, 3.85881885e-01, 3.26776734e-01,\n",
              "        6.53855812e-01, 2.11798016e-01],\n",
              "       [3.65482358e-02, 6.14666720e-01, 7.75597610e-01, 2.55959106e-02,\n",
              "        7.81422598e-01, 9.50951618e-01],\n",
              "       [8.41654630e-01, 6.77794175e-01, 9.21466356e-01, 3.19771357e-01,\n",
              "        4.86410075e-01, 4.44984343e-01],\n",
              "       [4.35707082e-01, 7.93688234e-01, 6.16011648e-02, 9.91346780e-01,\n",
              "        6.20013906e-01, 1.30934225e-01],\n",
              "       [9.25418935e-01, 1.83273061e-01, 2.53513980e-01, 9.41327621e-01,\n",
              "        4.90664602e-01, 1.24716651e-01],\n",
              "       [7.05419395e-01, 6.26376865e-01, 1.06979556e-01, 3.74560036e-01,\n",
              "        8.63305127e-01, 6.57485206e-01],\n",
              "       [8.78386889e-01, 5.51174440e-01, 6.15015776e-01, 6.85625963e-01,\n",
              "        5.23524701e-01, 1.45797066e-01],\n",
              "       [4.88343902e-02, 6.02085073e-01, 6.01318851e-01, 5.67099375e-01,\n",
              "        3.69752333e-01, 6.33249276e-01],\n",
              "       [9.95285266e-01, 6.95505126e-03, 2.29846805e-02, 4.94221034e-01,\n",
              "        2.24844967e-01, 5.98528017e-01],\n",
              "       [1.07359934e-01, 3.21341643e-01, 6.40645236e-01, 3.24950275e-01,\n",
              "        5.33061038e-01, 6.21455872e-01],\n",
              "       [4.58244637e-01, 2.05085160e-01, 2.54674874e-01, 4.93370923e-01,\n",
              "        9.97765851e-01, 1.76601950e-01],\n",
              "       [9.36527437e-01, 8.14569244e-01, 3.51398787e-01, 4.65277868e-01,\n",
              "        5.65027919e-01, 9.25429377e-01],\n",
              "       [5.95375946e-01, 1.94639671e-01, 1.94849025e-01, 3.86209575e-01,\n",
              "        1.26159078e-01, 3.74621210e-01],\n",
              "       [7.12057249e-01, 1.97720367e-01, 3.30688552e-01, 1.49774572e-01,\n",
              "        8.31393732e-01, 9.18371562e-01],\n",
              "       [2.77964958e-01, 7.98220541e-01, 8.58377555e-01, 5.66276103e-01,\n",
              "        9.73359318e-01, 8.51546634e-01],\n",
              "       [5.79251743e-01, 9.29932342e-02, 1.24717907e-01, 7.38365977e-01,\n",
              "        4.11408797e-01, 1.62486748e-01],\n",
              "       [2.08140189e-01, 6.95806568e-01, 9.40928469e-01, 4.26265273e-01,\n",
              "        7.62383824e-01, 9.24308498e-01],\n",
              "       [1.30674812e-01, 4.56458122e-01, 3.02877693e-01, 2.36465442e-01,\n",
              "        6.27390663e-01, 4.91799930e-01],\n",
              "       [9.82974556e-01, 2.82280231e-01, 1.60274329e-02, 8.30014998e-01,\n",
              "        8.30406964e-01, 6.27197374e-01],\n",
              "       [3.29824606e-02, 7.08975663e-01, 4.09120575e-02, 4.60443410e-01,\n",
              "        4.54937845e-01, 7.25721277e-01],\n",
              "       [2.58812558e-02, 2.60864004e-01, 5.58628320e-01, 1.08363703e-01,\n",
              "        8.08776836e-01, 8.60575121e-01],\n",
              "       [4.85289190e-01, 8.70502734e-02, 7.70186327e-01, 7.68796658e-01,\n",
              "        6.97884213e-01, 7.74727023e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Example probabilities for each modality\n",
        "text_probabilities = textemotion_pred\n",
        "audio_probabilities = videoemotion_pred\n",
        "video_probabilities = audioemotion_pred\n",
        "\n",
        "# Ground truth labels for the samples (replace with your actual labels)\n",
        "labels = finalemotion\n",
        "\n",
        "# Combine probabilities into a feature matrix\n",
        "X = np.concatenate([text_probabilities, audio_probabilities, video_probabilities], axis=1)\n",
        "\n",
        "# Define a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X.shape[1], activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, labels, epochs=50, verbose=0)\n",
        "\n",
        "# Get the learned weights\n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "# Display the learned weights\n",
        "textweight, audioweight, videoweight = weights.flatten()\n",
        "print(\"Learned Weights - Text: {:.3f}, Audio: {:.3f}, Video: {:.3f}\".format(textweight, audioweight, videoweight))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "d0wSk4vXgzIC",
        "outputId": "b3838d24-0318-4dfa-b4b3-36a6b3c83ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-d2f9de9b0682>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Combine probabilities into a feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_probabilities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Define a neural network model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tep=[]\n",
        "aep=[]\n",
        "vep=[]\n",
        "for i in textemotion_pred:\n",
        "  tep.append(i)\n",
        "for i in videoemotion_pred:\n",
        "  vep.append(i[0])\n",
        "for i in audioemotion_pred:\n",
        "  aep.append(i[0])"
      ],
      "metadata": {
        "id": "5FPimSMG_0Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Example probabilities for each modality\n",
        "text_probabilities = tep\n",
        "audio_probabilities = aep\n",
        "video_probabilities = vep\n",
        "\n",
        "# Ground truth labels for the samples (replace with your actual labels)\n",
        "labels = finalemotion\n",
        "\n",
        "# Combine probabilities into a feature matrix\n",
        "X = np.concatenate([text_probabilities, audio_probabilities, video_probabilities], axis=1)\n",
        "\n",
        "# Define a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=X.shape[1], activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, np.array(labels), epochs=100, verbose=0)\n",
        "\n",
        "# Get the learned weights\n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "# Display the learned weights\n",
        "textweight, audioweight, videoweight = weights.flatten()\n",
        "print(\"Learned Weights - Text: {:.3f}, Audio: {:.3f}, Video: {:.3f}\".format(textweight, audioweight, videoweight))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "dS3hwKj8vbF7",
        "outputId": "1fbbf55d-b8ec-4204-c0db-8170b7f4ab89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-2e51b1ffeac4>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Display the learned weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtextweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudioweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideoweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learned Weights - Text: {:.3f}, Audio: {:.3f}, Video: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudioweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideoweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(1,1,1) # aw =1.8\n",
        "getAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUVo7SWRFff8",
        "outputId": "8238ad6c-2dc4-42f1-b81d-83e23bb383ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " final emotion : 0.7439613526570048\n",
            " final emotion : 0.40096618357487923\n",
            " final emotion : 0.6231884057971014\n",
            " final emotion : 0.6376811594202898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tw=np.array(weights.flatten()[:6])\n",
        "aw=np.array(weights.flatten()[6:12])\n",
        "vw=np.array(weights.flatten()[12:])\n",
        "predict(tw,vw,aw) # aw =1.8\n",
        "getAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biwF1G02DZlA",
        "outputId": "c7dea258-cd74-4558-b1d4-e0a9d9daed55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " final emotion : 0.642512077294686\n",
            " final emotion : 0.23671497584541062\n",
            " final emotion : 0.5990338164251208\n",
            " final emotion : 0.5362318840579711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnzjSn9ixQ6u",
        "outputId": "96e3a574-d79c-4d32-aac3-0af31689fe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.29407823,  0.2788352 , -0.00676297, -0.00560845,  0.17745245,\n",
              "        0.06002386,  0.13825044, -0.08031727, -0.07798412,  0.17911737,\n",
              "        0.44740996, -0.15501039, -0.34821305, -0.00421693, -0.54174715,\n",
              "       -0.5254867 ,  0.29115033, -0.4537417 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape\n",
        "\n",
        "# Example probabilities for each modality\n",
        "text_probabilities = np.random.rand(132, 6)\n",
        "audio_probabilities = np.random.rand(132, 6)\n",
        "video_probabilities = np.random.rand(132, 6)\n",
        "\n",
        "# Ground truth labels for the samples (replace with your actual labels)\n",
        "labels = np.random.randint(2, size=(132,))\n",
        "\n",
        "# Concatenate probabilities into a feature matrix\n",
        "X = np.concatenate([text_probabilities, audio_probabilities, video_probabilities], axis=1)\n",
        "\n",
        "# Convert the feature matrix to float32\n",
        "X = X.astype(np.float32)\n",
        "\n",
        "# Define a neural network model with trainable weights\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "weights = Dense(18, activation='softmax', use_bias=False, dtype=tf.float32)(input_layer)\n",
        "\n",
        "# Reshape the weights to (18, 1)\n",
        "weights_reshaped = Reshape((18, 1))(weights)\n",
        "\n",
        "# Repeat the weights for each sample\n",
        "weights_repeated = tf.tile(weights_reshaped, [1, 1])\n",
        "\n",
        "# Reshape X to match the dimensions for multiplication\n",
        "X_reshaped = Reshape((tf.shape(X)[0], 3, 6))(X)\n",
        "\n",
        "# Permute dimensions for correct multiplication\n",
        "X_permuted = tf.transpose(X_reshaped, perm=[0, 2, 1, 3])\n",
        "\n",
        "# Apply weights to the probabilities\n",
        "weighted_probabilities = tf.math.multiply(X_permuted, weights_repeated)\n",
        "\n",
        "# Sum the weighted probabilities along axis 3\n",
        "final_pred = Dense(1, activation='sigmoid')(tf.reduce_sum(weighted_probabilities, axis=3))\n",
        "\n",
        "# Build the model\n",
        "model = Model(inputs=input_layer, outputs=final_pred)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, labels, epochs=100, verbose=0)\n",
        "\n",
        "# Get the learned weights\n",
        "weights_value = model.layers[1].get_weights()[0]\n",
        "\n",
        "# Display the learned weights\n",
        "print(\"Learned Weights:\", weights_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "bgtzHQUwxjL4",
        "outputId": "1c39fbb2-9cb1-43cc-9e5f-bde0a8d535f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-0d66bcbb3141>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Repeat the weights for each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mweights_repeated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Reshape X to match the dimensions for multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  12029\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12031\u001b[0;31m       _result = _dispatch.dispatch(\n\u001b[0m\u001b[1;32m  12032\u001b[0m             \u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12033\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(op, args, kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdispatcher\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GLOBAL_DISPATCHERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/tf_op_layer.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         ):\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mTFOpLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.tile_9\" (type TFOpLambda).\n\nShape must be rank 3 but is rank 2 for '{{node tf.tile_9/Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](Placeholder, tf.tile_9/Tile/multiples)' with input shapes: [?,18,1], [2].\n\nCall arguments received by layer \"tf.tile_9\" (type TFOpLambda):\n   input=tf.Tensor(shape=(None, 18, 1), dtype=float32)\n   multiples=['1', '1']\n   name=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrmDReUEDM_E",
        "outputId": "ef7a7662-93ea-4656-cfaa-35f6c0bb7af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
              "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gDjCk-gDQ-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgqJUUz5rhDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FNpvPmjgrhAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Specify the path for the new folder\n",
        "folder_path = \"/content/AudioWithCategorisedWAV/\"\n",
        "\n",
        "folders = ['Anger','Disgust','Fear','Happy','Neutral','Sad']\n",
        "\n",
        "for i in folders:\n",
        "  folder_path1 = folder_path + i\n",
        "  if not os.path.exists(folder_path1):\n",
        "    os.makedirs(folder_path1)\n",
        "\n",
        "import os\n",
        "\n",
        "import shutil\n",
        "#sourcepath = '/content/AudioInput'\n",
        "sourcepath = '/content/Audio/AudioWAV'\n",
        "destinationpath = '/content/AudioWithCategorisedWAV'\n",
        "files = os.listdir(sourcepath)\n",
        "\n",
        "for file in files:\n",
        "    source_file = os.path.join(sourcepath, file)\n",
        "    if not '.wav' in file:\n",
        "      continue\n",
        "    if 'ANG' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Anger', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'DIS' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Disgust', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'FEA' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Fear', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'HAP' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Happy', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'NEU' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Neutral', file)\n",
        "      shutil.move(source_file, destination_file)\n",
        "    elif 'SAD' in file:\n",
        "      destination_file = os.path.join(destinationpath+'/Sad', file)\n",
        "      shutil.move(source_file, destination_file)"
      ],
      "metadata": {
        "id": "PnGLPPsmrg9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.70):\n",
        "    return librosa.effects.time_stretch(data, rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.8):\n",
        "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
        "\n",
        "def higher_speed(data, speed_factor = 1.25):\n",
        "    return librosa.effects.time_stretch(data, rate = speed_factor)\n",
        "\n",
        "def lower_speed(data, speed_factor = 0.75):\n",
        "    return librosa.effects.time_stretch(data, rate = speed_factor)"
      ],
      "metadata": {
        "id": "zxSMTnRyronJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to extract audio features using librosa\n",
        "def extract_features(audio, sample_rate, mfcc=True, chroma=True, mel=True):\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13), axis=1)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    return result\n",
        "\n",
        "# Function to load audio data and labels\n",
        "def load_data(data_path):\n",
        "    features, labels = [], []\n",
        "    for folder in os.listdir(data_path):\n",
        "        label = folder\n",
        "        for file_name in os.listdir(os.path.join(data_path, folder)):\n",
        "            file_path = os.path.join(data_path, folder, file_name)\n",
        "            audio, sample_rate = librosa.load(file_path)\n",
        "            feature = extract_features(audio,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "            #noised\n",
        "            noise_data = noise(audio)\n",
        "            feature = extract_features(noise_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "\n",
        "            #speed up\n",
        "            higher_speed_data = higher_speed(audio)\n",
        "            feature = extract_features(higher_speed_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "\n",
        "            #speed down\n",
        "            lower_speed_data = higher_speed(audio)\n",
        "            feature = extract_features(lower_speed_data,sample_rate)\n",
        "            features.append(feature)\n",
        "            labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load data and preprocess\n",
        "data_path = \"/content/AudioWithCategorisedWAV\"\n",
        "features, labels = load_data(data_path)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "\n"
      ],
      "metadata": {
        "id": "4ByDFW2HrqZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.1, random_state=42)\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(layers.Conv1D(128, kernel_size=3, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "#model.add(layers.Conv1D(256, kernel_size=3, activation='relu'))\n",
        "#model.add(layers.BatchNormalization())\n",
        "#model.add(layers.MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Recurrent layers\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
        "model.add(layers.Bidirectional(layers.LSTM(128)))\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(layers.Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],run_eagerly=True)"
      ],
      "metadata": {
        "id": "_J4o2yqwrxSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_labels = np.unique(labels)\n",
        "class_indices = {label: index for index, label in enumerate(class_labels)}\n",
        "Y = np.array([class_indices[label] for label in labels])\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(class_weight ='balanced',classes = np.unique(Y),y= Y)\n",
        "\n",
        "# Convert class weights to a dictionary for class_weight parameter in model.fit\n",
        "class_weights_dict = {class_index: weight for class_index, weight in zip(np.unique(Y), class_weights)}\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), class_weight=class_weights_dict)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhgbW8sIrzE4",
        "outputId": "14a69801-b728-4db5-ead9-13f39a5fbd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7c4a5dfb5240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7c4a5dfb5240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - 116s 127ms/step - loss: 1.5264 - accuracy: 0.3645 - val_loss: 1.4605 - val_accuracy: 0.3890\n",
            "Epoch 2/50\n",
            "838/838 [==============================] - 108s 128ms/step - loss: 1.4718 - accuracy: 0.3864 - val_loss: 1.4750 - val_accuracy: 0.3880\n",
            "Epoch 3/50\n",
            "838/838 [==============================] - 108s 129ms/step - loss: 1.4504 - accuracy: 0.4029 - val_loss: 1.4649 - val_accuracy: 0.3853\n",
            "Epoch 4/50\n",
            "838/838 [==============================] - 109s 130ms/step - loss: 1.4386 - accuracy: 0.4101 - val_loss: 1.4003 - val_accuracy: 0.4189\n",
            "Epoch 5/50\n",
            "838/838 [==============================] - 107s 127ms/step - loss: 1.4165 - accuracy: 0.4164 - val_loss: 1.3962 - val_accuracy: 0.4259\n",
            "Epoch 6/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 1.3970 - accuracy: 0.4272 - val_loss: 1.5457 - val_accuracy: 0.3665\n",
            "Epoch 7/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 1.3849 - accuracy: 0.4357 - val_loss: 1.3763 - val_accuracy: 0.4327\n",
            "Epoch 8/50\n",
            "838/838 [==============================] - 107s 128ms/step - loss: 1.3604 - accuracy: 0.4447 - val_loss: 1.3726 - val_accuracy: 0.4236\n",
            "Epoch 9/50\n",
            "838/838 [==============================] - 110s 132ms/step - loss: 1.3408 - accuracy: 0.4472 - val_loss: 1.3875 - val_accuracy: 0.4175\n",
            "Epoch 10/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 1.3199 - accuracy: 0.4566 - val_loss: 1.3548 - val_accuracy: 0.4387\n",
            "Epoch 11/50\n",
            "838/838 [==============================] - 105s 126ms/step - loss: 1.2943 - accuracy: 0.4743 - val_loss: 1.3225 - val_accuracy: 0.4478\n",
            "Epoch 12/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 1.2709 - accuracy: 0.4818 - val_loss: 1.2882 - val_accuracy: 0.4693\n",
            "Epoch 13/50\n",
            "838/838 [==============================] - 106s 127ms/step - loss: 1.2317 - accuracy: 0.4959 - val_loss: 1.2710 - val_accuracy: 0.4770\n",
            "Epoch 14/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 1.1994 - accuracy: 0.5092 - val_loss: 1.2942 - val_accuracy: 0.4763\n",
            "Epoch 15/50\n",
            "838/838 [==============================] - 105s 126ms/step - loss: 1.1661 - accuracy: 0.5245 - val_loss: 1.2294 - val_accuracy: 0.5008\n",
            "Epoch 16/50\n",
            "838/838 [==============================] - 106s 127ms/step - loss: 1.1270 - accuracy: 0.5379 - val_loss: 1.2027 - val_accuracy: 0.5059\n",
            "Epoch 17/50\n",
            "838/838 [==============================] - 107s 127ms/step - loss: 1.0936 - accuracy: 0.5587 - val_loss: 1.2334 - val_accuracy: 0.5032\n",
            "Epoch 18/50\n",
            "838/838 [==============================] - 103s 122ms/step - loss: 1.0488 - accuracy: 0.5759 - val_loss: 1.1538 - val_accuracy: 0.5257\n",
            "Epoch 19/50\n",
            "838/838 [==============================] - 104s 124ms/step - loss: 1.0011 - accuracy: 0.5984 - val_loss: 1.1884 - val_accuracy: 0.5361\n",
            "Epoch 20/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.9655 - accuracy: 0.6105 - val_loss: 1.1173 - val_accuracy: 0.5569\n",
            "Epoch 21/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 0.9185 - accuracy: 0.6332 - val_loss: 1.1088 - val_accuracy: 0.5600\n",
            "Epoch 22/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.8842 - accuracy: 0.6495 - val_loss: 0.9982 - val_accuracy: 0.6003\n",
            "Epoch 23/50\n",
            "838/838 [==============================] - 104s 124ms/step - loss: 0.8398 - accuracy: 0.6654 - val_loss: 1.0289 - val_accuracy: 0.5915\n",
            "Epoch 24/50\n",
            "838/838 [==============================] - 105s 126ms/step - loss: 0.8144 - accuracy: 0.6769 - val_loss: 1.0714 - val_accuracy: 0.5895\n",
            "Epoch 25/50\n",
            "838/838 [==============================] - 104s 124ms/step - loss: 0.7796 - accuracy: 0.6880 - val_loss: 1.0083 - val_accuracy: 0.6127\n",
            "Epoch 26/50\n",
            "838/838 [==============================] - 104s 125ms/step - loss: 0.7397 - accuracy: 0.7097 - val_loss: 1.0603 - val_accuracy: 0.6030\n",
            "Epoch 27/50\n",
            "838/838 [==============================] - 108s 129ms/step - loss: 0.7081 - accuracy: 0.7179 - val_loss: 0.9594 - val_accuracy: 0.6379\n",
            "Epoch 28/50\n",
            "838/838 [==============================] - 108s 129ms/step - loss: 0.6896 - accuracy: 0.7297 - val_loss: 0.9225 - val_accuracy: 0.6765\n",
            "Epoch 29/50\n",
            "838/838 [==============================] - 111s 133ms/step - loss: 0.6564 - accuracy: 0.7448 - val_loss: 0.8793 - val_accuracy: 0.6910\n",
            "Epoch 30/50\n",
            "838/838 [==============================] - 108s 129ms/step - loss: 0.6410 - accuracy: 0.7517 - val_loss: 0.9130 - val_accuracy: 0.6772\n",
            "Epoch 31/50\n",
            "838/838 [==============================] - 106s 127ms/step - loss: 0.6094 - accuracy: 0.7613 - val_loss: 0.9875 - val_accuracy: 0.6564\n",
            "Epoch 32/50\n",
            "838/838 [==============================] - 105s 125ms/step - loss: 0.5865 - accuracy: 0.7729 - val_loss: 0.8387 - val_accuracy: 0.7175\n",
            "Epoch 33/50\n",
            "838/838 [==============================] - 106s 127ms/step - loss: 0.5815 - accuracy: 0.7747 - val_loss: 0.8866 - val_accuracy: 0.6957\n",
            "Epoch 34/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 0.5543 - accuracy: 0.7872 - val_loss: 0.8602 - val_accuracy: 0.6994\n",
            "Epoch 35/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 0.5315 - accuracy: 0.7926 - val_loss: 0.8728 - val_accuracy: 0.7004\n",
            "Epoch 36/50\n",
            "838/838 [==============================] - 105s 125ms/step - loss: 0.5457 - accuracy: 0.7910 - val_loss: 0.7937 - val_accuracy: 0.7336\n",
            "Epoch 37/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.5039 - accuracy: 0.8044 - val_loss: 0.7911 - val_accuracy: 0.7299\n",
            "Epoch 38/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.4922 - accuracy: 0.8110 - val_loss: 0.8042 - val_accuracy: 0.7414\n",
            "Epoch 39/50\n",
            "838/838 [==============================] - 104s 124ms/step - loss: 0.4836 - accuracy: 0.8153 - val_loss: 0.8323 - val_accuracy: 0.7370\n",
            "Epoch 40/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 0.4716 - accuracy: 0.8208 - val_loss: 0.8605 - val_accuracy: 0.7333\n",
            "Epoch 41/50\n",
            "838/838 [==============================] - 106s 126ms/step - loss: 0.4459 - accuracy: 0.8302 - val_loss: 0.7805 - val_accuracy: 0.7595\n",
            "Epoch 42/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.4472 - accuracy: 0.8290 - val_loss: 0.8899 - val_accuracy: 0.7330\n",
            "Epoch 43/50\n",
            "838/838 [==============================] - 132s 158ms/step - loss: 0.4277 - accuracy: 0.8371 - val_loss: 0.9298 - val_accuracy: 0.7199\n",
            "Epoch 44/50\n",
            "838/838 [==============================] - 105s 126ms/step - loss: 0.4269 - accuracy: 0.8369 - val_loss: 0.8284 - val_accuracy: 0.7484\n",
            "Epoch 45/50\n",
            "838/838 [==============================] - 105s 125ms/step - loss: 0.4112 - accuracy: 0.8446 - val_loss: 0.7842 - val_accuracy: 0.7702\n",
            "Epoch 46/50\n",
            "838/838 [==============================] - 102s 122ms/step - loss: 0.3955 - accuracy: 0.8502 - val_loss: 0.9687 - val_accuracy: 0.7094\n",
            "Epoch 47/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 0.4066 - accuracy: 0.8446 - val_loss: 0.8916 - val_accuracy: 0.7504\n",
            "Epoch 48/50\n",
            "838/838 [==============================] - 104s 124ms/step - loss: 0.3751 - accuracy: 0.8594 - val_loss: 0.8584 - val_accuracy: 0.7655\n",
            "Epoch 49/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 0.3870 - accuracy: 0.8549 - val_loss: 0.8597 - val_accuracy: 0.7565\n",
            "Epoch 50/50\n",
            "838/838 [==============================] - 103s 123ms/step - loss: 0.3711 - accuracy: 0.8587 - val_loss: 0.8585 - val_accuracy: 0.7581\n",
            "94/94 [==============================] - 5s 57ms/step - loss: 0.8585 - accuracy: 0.7581\n",
            "Test accuracy: 0.758145809173584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"emotion_classification_Audio_model_With_Augmentation.h5\")"
      ],
      "metadata": {
        "id": "ZupRgVqWr1bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZatU9FtF0H9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}